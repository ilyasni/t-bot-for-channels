---
title: "RAG System"
description: "–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫, AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è, –¥–∞–π–¥–∂–µ—Å—Ç—ã, Qdrant"
tags: ["rag", "vector-search", "ai", "qdrant", "embeddings"]
version: "3.1"
---

# RAG System

## üéØ High-Level Overview

**RAG Service** ‚Äî –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤.

**Stack:**
- Qdrant (–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î)
- GigaChat (embeddings)
- Redis (cache)
- OpenRouter (LLM generation)

```
User Query ‚Üí Embedding ‚Üí Qdrant Search ‚Üí Top K Results
                                              ‚Üì
                                    Context + Query ‚Üí LLM ‚Üí Answer
```

## üèóÔ∏è RAG Architecture

### Pipeline

```python
# 1. –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ)
Post ‚Üí Embedding (GigaChat) ‚Üí Qdrant Vector ‚Üí Indexed
     ‚Üì (–∫–µ—à)
   Redis (24h TTL)

# 2. –ü–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
User Query ‚Üí Embedding ‚Üí Qdrant Search ‚Üí Top K Results
                                              ‚Üì
                                    Context + Query ‚Üí LLM ‚Üí Answer
```

### –ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å

```yaml
# docker-compose.override.yml
rag-service:
  build: ./telethon/rag_service
  ports:
    - "8020:8020"
  environment:
    - QDRANT_URL=http://qdrant:6333
    - REDIS_HOST=redis
    - GIGACHAT_PROXY_URL=http://gpt2giga-proxy:8090
```

## üì¶ Components

### 1. Vector DB (Qdrant)

```python
# rag_service/vector_db.py
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, Filter, FieldCondition

class VectorDB:
    def __init__(self):
        self.client = QdrantClient(
            url=os.getenv("QDRANT_URL", "http://qdrant:6333"),
            api_key=os.getenv("QDRANT_API_KEY")
        )
    
    async def create_collection(self, user_id: int):
        """
        –°–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        """
        collection_name = f"user_{user_id}_posts"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è
        collections = self.client.get_collections().collections
        if collection_name in [c.name for c in collections]:
            return
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        self.client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=1536,  # GigaChat embeddings dimension
                distance=Distance.COSINE
            )
        )
    
    async def upsert_post(
        self,
        user_id: int,
        post_id: int,
        embedding: List[float],
        metadata: dict
    ):
        """
        –î–æ–±–∞–≤–∏—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å –ø–æ—Å—Ç
        """
        collection_name = f"user_{user_id}_posts"
        
        # Metadata –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        payload = {
            "post_id": post_id,
            "channel_id": metadata.get("channel_id"),
            "channel_username": metadata.get("channel_username"),
            "tags": metadata.get("tags", []),
            "posted_at": metadata.get("posted_at"),
            "url": metadata.get("url"),
            "text": metadata.get("text")[:500]  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
        }
        
        self.client.upsert(
            collection_name=collection_name,
            points=[{
                "id": post_id,
                "vector": embedding,
                "payload": payload
            }]
        )
    
    async def search(
        self,
        user_id: int,
        query_vector: List[float],
        filters: Optional[dict] = None,
        top_k: int = 10,
        score_threshold: float = 0.7
    ) -> List[dict]:
        """
        –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        """
        collection_name = f"user_{user_id}_posts"
        
        # –§–∏–ª—å—Ç—Ä—ã
        query_filter = None
        if filters:
            conditions = []
            
            if "channel_id" in filters:
                conditions.append(
                    FieldCondition(
                        key="channel_id",
                        match={"value": filters["channel_id"]}
                    )
                )
            
            if "tags" in filters:
                conditions.append(
                    FieldCondition(
                        key="tags",
                        match={"any": filters["tags"]}
                    )
                )
            
            if conditions:
                query_filter = Filter(must=conditions)
        
        # –ü–æ–∏—Å–∫
        results = self.client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            query_filter=query_filter,
            limit=top_k,
            score_threshold=score_threshold
        )
        
        return [
            {
                "post_id": r.id,
                "score": r.score,
                **r.payload
            }
            for r in results
        ]
```

### 2. Embeddings Service

```python
# rag_service/embeddings.py
import httpx
import hashlib
import json
from typing import List

class EmbeddingsService:
    def __init__(self):
        self.gigachat_url = os.getenv(
            "GIGACHAT_PROXY_URL",
            "http://gpt2giga-proxy:8090"
        )
        
        # Redis cache
        self.redis_client = redis.Redis(
            host=os.getenv("REDIS_HOST", "redis"),
            port=6379,
            decode_responses=True
        )
        self.cache_ttl = 86400  # 24 hours
    
    async def generate(self, text: str) -> List[float]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        # Cache key
        text_hash = hashlib.md5(text.encode()).hexdigest()
        cache_key = f"embedding:{text_hash}"
        
        # Check cache
        cached = self.redis_client.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # Generate —á–µ—Ä–µ–∑ GigaChat
        embedding = await self._generate_gigachat(text)
        
        # Save to cache
        self.redis_client.setex(
            cache_key,
            self.cache_ttl,
            json.dumps(embedding)
        )
        
        return embedding
    
    async def _generate_gigachat(self, text: str) -> List[float]:
        """
        GigaChat embeddings (EmbeddingsGigaR)
        """
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.gigachat_url}/v1/embeddings",
                json={
                    "model": "EmbeddingsGigaR",
                    "input": text
                },
                timeout=30.0
            )
            
            response.raise_for_status()
            data = response.json()
            
            return data["data"][0]["embedding"]
```

### 3. RAG Generator

```python
# rag_service/generator.py
from typing import List, Dict, Any

class RAGGenerator:
    def __init__(self):
        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        self.model = os.getenv("OPENROUTER_MODEL", "google/gemini-2.0-flash-exp:free")
    
    async def generate_answer(
        self,
        query: str,
        context_posts: List[dict],
        max_tokens: int = 500,
        temperature: float = 0.3
    ) -> Dict[str, Any]:
        """
        RAG-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        """
        # 1. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_text = self._format_context(context_posts)
        
        # 2. Prompt –¥–ª—è LLM
        prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ Telegram –∫–∞–Ω–∞–ª–æ–≤:
{context_text}

–í–æ–ø—Ä–æ—Å: {query}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
- –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
- –£–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–∫–∞–Ω–∞–ª—ã) –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –≤–∑—è—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

–û—Ç–≤–µ—Ç:"""
        
        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenRouter
        answer = await self._generate_openrouter(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        # 4. Prepare response
        return {
            "answer": answer,
            "sources": [
                {
                    "post_id": p["post_id"],
                    "channel": p["channel_username"],
                    "url": p["url"],
                    "score": p["score"]
                }
                for p in context_posts
            ],
            "context_size": len(context_posts)
        }
    
    def _format_context(self, posts: List[dict]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        formatted = []
        
        for i, post in enumerate(posts, 1):
            formatted.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: @{post['channel_username']}, "
                f"—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {post['score']:.2f}]\n"
                f"{post['text']}\n"
            )
        
        return "\n---\n".join(formatted)
    
    async def _generate_openrouter(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> str:
        """
        OpenRouter API call
        """
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.openrouter_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": max_tokens,
                    "temperature": temperature
                },
                timeout=60.0
            )
            
            response.raise_for_status()
            data = response.json()
            
            return data["choices"][0]["message"]["content"]
```

### 4. Indexer Service

```python
# rag_service/indexer.py
from typing import Optional

class IndexerService:
    def __init__(self):
        self.vector_db = VectorDB()
        self.embeddings_service = EmbeddingsService()
    
    async def index_post(self, post_id: int, user_id: int) -> bool:
        """
        –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç
        """
        # 1. –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç –∏–∑ –ë–î
        db = SessionLocal()
        post = db.query(Post).filter(Post.id == post_id).first()
        
        if not post:
            db.close()
            return False
        
        # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding
        text = post.text
        if post.enriched_content:
            text = f"{post.text}\n\n{post.enriched_content}"
        
        embedding = await self.embeddings_service.generate(text)
        
        # 3. Metadata
        metadata = {
            "channel_id": post.channel_id,
            "channel_username": post.channel.channel_username,
            "tags": post.tags or [],
            "posted_at": post.posted_at.isoformat(),
            "url": post.url,
            "text": text
        }
        
        # 4. Upsert –≤ Qdrant
        await self.vector_db.upsert_post(
            user_id=user_id,
            post_id=post.id,
            embedding=embedding,
            metadata=metadata
        )
        
        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –ë–î
        status = IndexingStatus(
            user_id=user_id,
            post_id=post.id,
            vector_id=f"user_{user_id}_posts:{post.id}",
            status="success"
        )
        db.add(status)
        db.commit()
        db.close()
        
        return True
    
    async def batch_index(
        self,
        user_id: int,
        post_ids: List[int]
    ) -> Dict[str, int]:
        """
        Batch –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è (–¥–æ 100 –ø–æ—Å—Ç–æ–≤)
        """
        success = 0
        failed = 0
        
        for batch in chunks(post_ids, 100):
            tasks = [
                self.index_post(post_id, user_id)
                for post_id in batch
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    failed += 1
                elif result:
                    success += 1
                else:
                    failed += 1
        
        return {"success": success, "failed": failed}
```

## üöÄ RAG API Endpoints

```python
# rag_service/main.py
from fastapi import FastAPI, HTTPException, BackgroundTasks

app = FastAPI(title="RAG Service")

@app.post("/rag/query")
async def rag_query(
    user_id: int,
    query: str,
    top_k: int = 10,
    min_score: float = 0.7
):
    """
    RAG-–∑–∞–ø—Ä–æ—Å
    """
    # 1. Embedding –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = await embeddings_service.generate(query)
    
    # 2. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
    results = await vector_db.search(
        user_id=user_id,
        query_vector=query_embedding,
        top_k=top_k,
        score_threshold=min_score
    )
    
    if not results:
        return {
            "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≤–∞—à–∏—Ö –∫–∞–Ω–∞–ª–∞—Ö.",
            "sources": []
        }
    
    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    response = await rag_generator.generate_answer(
        query=query,
        context_posts=results
    )
    
    return response

@app.post("/rag/index/post/{post_id}")
async def index_post_endpoint(
    post_id: int,
    user_id: int,
    background_tasks: BackgroundTasks
):
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç (background)
    """
    background_tasks.add_task(
        indexer_service.index_post,
        post_id,
        user_id
    )
    
    return {"status": "queued", "post_id": post_id}

@app.post("/rag/index/batch")
async def batch_index_endpoint(
    user_id: int,
    post_ids: List[int],
    background_tasks: BackgroundTasks
):
    """
    Batch –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    """
    background_tasks.add_task(
        indexer_service.batch_index,
        user_id,
        post_ids
    )
    
    return {"status": "queued", "count": len(post_ids)}
```

## ‚úÖ Verification Steps

1. **Qdrant –¥–æ—Å—Ç—É–ø–µ–Ω:**
```bash
curl http://localhost:6333/collections
# {"result": {"collections": [...]}}
```

2. **–ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞:**
```python
collections = qdrant_client.get_collections()
user_collections = [c.name for c in collections if "user_" in c.name]
assert len(user_collections) > 0
```

3. **Embeddings –∫–µ—à–∏—Ä—É—é—Ç—Å—è:**
```bash
docker exec redis redis-cli KEYS "embedding:*"
# –î–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑–∞—Ç—å –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings
```

4. **–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç:**
```python
result = await indexer_service.index_post(post_id=1, user_id=1)
assert result == True
```

5. **RAG query —Ä–∞–±–æ—Ç–∞–µ—Ç:**
```bash
curl -X POST http://localhost:8020/rag/query \
  -H "Content-Type: application/json" \
  -d '{"user_id": 1, "query": "AI –Ω–æ–≤–æ—Å—Ç–∏"}'
# {"answer": "...", "sources": [...]}
```

## üö® Troubleshooting

### RAG –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Å—Ç—ã

```bash
# 1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
curl http://localhost:8020/rag/index/status/1

# 2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Qdrant collection
curl http://localhost:6333/collections/user_1_posts

# 3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ embeddings cache
docker exec redis redis-cli KEYS "embedding:*"
```

### GigaChat embeddings –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç

```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ gpt2giga-proxy
curl http://localhost:8090/v1/models
# –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
```

## üìö –°–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞

- `01-core.mdc` ‚Äî –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
- `03-database.mdc` ‚Äî Redis cache
- `09-external.mdc` ‚Äî Qdrant, GigaChat
