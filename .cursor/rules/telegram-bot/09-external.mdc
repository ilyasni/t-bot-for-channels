---
title: "External Services Integration"
description: "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Qdrant, Searxng, Crawl4AI, Ollama, n8n"
tags: ["external", "integration", "qdrant", "crawl4ai", "ollama"]
version: "3.1"
---

# External Services Integration

## üéØ High-Level Overview

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:

- **Qdrant** ‚Äî –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î
- **Redis/Valkey** ‚Äî –∫–µ—à + sessions
- **Searxng** ‚Äî –º–µ—Ç–∞–ø–æ–∏—Å–∫–æ–≤–∏–∫
- **Crawl4AI** ‚Äî –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥
- **Ollama** ‚Äî –ª–æ–∫–∞–ª—å–Ω—ã–µ LLM
- **n8n/Flowise** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è

## üîç Qdrant (Vector Database)

### Configuration

```python
# rag_service/vector_db.py
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

class QdrantConfig:
    # URLs
    INTERNAL_URL = "http://qdrant:6333"  # Inside Docker
    EXTERNAL_URL = "https://qdrant.produman.studio"  # Public
    
    # API Key
    API_KEY = os.getenv("QDRANT_API_KEY")
    
    # Vector config
    VECTOR_SIZE = 1536  # GigaChat embeddings
    DISTANCE = Distance.COSINE
    
    # Performance
    BATCH_SIZE = 100  # Max points per upsert
    TIMEOUT = 30  # Seconds

# Client initialization
qdrant_client = QdrantClient(
    url=QdrantConfig.INTERNAL_URL,
    api_key=QdrantConfig.API_KEY,
    timeout=QdrantConfig.TIMEOUT
)
```

### Best Practices

```python
# 1. –ò–∑–æ–ª—è—Ü–∏—è –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
collection_name = f"user_{user_id}_posts"

# 2. Metadata –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
payload = {
    "channel_id": channel.id,
    "tags": post.tags,
    "posted_at": post.posted_at.isoformat()
}

# 3. Batch operations
points = [
    {
        "id": post.id,
        "vector": embedding,
        "payload": metadata
    }
    for post in posts[:100]  # Max 100 at once
]

qdrant_client.upsert(
    collection_name=collection_name,
    points=points
)

# 4. Score threshold
results = qdrant_client.search(
    collection_name=collection_name,
    query_vector=query_embedding,
    limit=10,
    score_threshold=0.7  # Minimum relevance
)
```

## üî¥ Redis/Valkey (Cache + Sessions)

### Configuration

```python
import redis
import json

# Connection (–ë–ï–ó –ø–∞—Ä–æ–ª—è!)
redis_client = redis.Redis(
    host=os.getenv("REDIS_HOST", "redis"),
    port=int(os.getenv("REDIS_PORT", 6379)),
    decode_responses=True
)

# Healthcheck
try:
    redis_client.ping()
    print("‚úÖ Redis connected")
except redis.ConnectionError as e:
    print(f"‚ùå Redis unavailable: {e}")
```

### Use Cases

```python
# 1. Embeddings Cache (24h)
EMBEDDING_TTL = 86400

def cache_embedding(text_hash: str, embedding: List[float]):
    redis_client.setex(
        f"embedding:{text_hash}",
        EMBEDDING_TTL,
        json.dumps(embedding)
    )

def get_cached_embedding(text_hash: str) -> Optional[List[float]]:
    cached = redis_client.get(f"embedding:{text_hash}")
    return json.loads(cached) if cached else None

# 2. QR Sessions (10 –º–∏–Ω—É—Ç)
QR_SESSION_TTL = 600

def save_qr_session(session_id: str, data: dict):
    redis_client.setex(
        f"qr_session:{session_id}",
        QR_SESSION_TTL,
        json.dumps(data)
    )

# 3. Admin Sessions (1 —á–∞—Å)
ADMIN_SESSION_TTL = 3600

def save_admin_session(token: str, admin_data: dict):
    redis_client.setex(
        f"admin_session:{token}",
        ADMIN_SESSION_TTL,
        json.dumps(admin_data)
    )

# 4. Rate Limiting (1 –º–∏–Ω—É—Ç–∞)
RATE_LIMIT_WINDOW = 60

def check_rate_limit(user_id: int, endpoint: str, limit: int) -> bool:
    key = f"rate:{user_id}:{endpoint}"
    current = redis_client.incr(key)
    
    if current == 1:
        redis_client.expire(key, RATE_LIMIT_WINDOW)
    
    return current <= limit
```

### TTL Policy

```python
TTL_POLICY = {
    "embeddings": 86400,      # 24 hours
    "rag_answers": 3600,      # 1 hour
    "qr_sessions": 600,       # 10 minutes
    "admin_sessions": 3600,   # 1 hour
    "rate_limits": 60         # 1 minute
}
```

## üîé Searxng (Meta Search Engine)

### Configuration

```python
class SearxngConfig:
    URL = os.getenv("SEARXNG_URL", "https://searxng.produman.studio")
    USERNAME = os.getenv("SEARXNG_USER")
    PASSWORD = os.getenv("SEARXNG_PASSWORD")
    TIMEOUT = 10  # Seconds
    MAX_RESULTS = 5  # Per query

# HTTP client —Å auth
searxng_auth = httpx.BasicAuth(
    SearxngConfig.USERNAME,
    SearxngConfig.PASSWORD
)
```

### Usage in RAG

```python
async def enhance_rag_with_web(query: str) -> List[str]:
    """
    –†–∞—Å—à–∏—Ä–∏—Ç—å RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–µ–±-–ø–æ–∏—Å–∫–æ–º
    """
    async with httpx.AsyncClient(
        auth=searxng_auth,
        timeout=SearxngConfig.TIMEOUT
    ) as client:
        response = await client.get(
            f"{SearxngConfig.URL}/search",
            params={
                "q": query,
                "format": "json",
                "categories": "general",
                "engines": "google,bing,duckduckgo",
                "language": "ru"
            }
        )
        
        response.raise_for_status()
        results = response.json()["results"][:SearxngConfig.MAX_RESULTS]
        
        return [
            r["content"]
            for r in results
            if r.get("content")
        ]

# Integration
async def hybrid_search(user_id: int, query: str):
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –ø–æ—Å—Ç—ã + –≤–µ–±
    """
    # 1. –ü–æ–∏—Å–∫ –≤ –ø–æ—Å—Ç–∞—Ö (RAG)
    posts_results = await rag_service.search(user_id, query)
    
    # 2. –ü–æ–∏—Å–∫ –≤ –≤–µ–±–µ (Searxng)
    web_results = await enhance_rag_with_web(query)
    
    # 3. Combine context
    context = {
        "posts": posts_results,
        "web": web_results
    }
    
    # 4. Generate answer
    return await rag_generator.generate_answer(query, context)
```

## üï∑Ô∏è Crawl4AI (Web Scraping)

### Configuration

```python
class Crawl4AIConfig:
    URL = "http://crawl4ai:11235"
    TIMEOUT = 30  # Seconds
    WORD_COUNT_THRESHOLD = 100  # Minimum words
    MAX_LINKS_PER_POST = 10
```

### Post Enrichment

```python
async def enrich_post_with_links(post: Post):
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Å—Å—ã–ª–æ–∫
    """
    # 1. Extract URLs
    urls = extract_urls(post.text)
    
    if not urls:
        return
    
    # 2. Crawl URLs (–¥–æ 10)
    enriched_content = []
    
    for url in urls[:Crawl4AIConfig.MAX_LINKS_PER_POST]:
        try:
            async with httpx.AsyncClient(timeout=Crawl4AIConfig.TIMEOUT) as client:
                response = await client.post(
                    f"{Crawl4AIConfig.URL}/crawl",
                    json={
                        "url": url,
                        "word_count_threshold": Crawl4AIConfig.WORD_COUNT_THRESHOLD,
                        "only_text": True
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    content = data.get("markdown", "")
                    
                    if content:
                        enriched_content.append(f"[{url}]\n{content}")
        
        except Exception as e:
            logger.warning(f"Failed to crawl {url}: {e}")
            continue
    
    # 3. Save enriched_content
    if enriched_content:
        post.enriched_content = "\n\n---\n\n".join(enriched_content)
        db.commit()
        
        # 4. Re-index —Å –Ω–æ–≤—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        await rag_service.reindex_post(post.id)

# Usage
async def parse_channel_with_enrichment(channel_id: int):
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–±–æ–≥–∞—â–µ–Ω–∏–µ–º
    """
    posts = await fetch_channel_posts(channel_id)
    
    for post in posts:
        # Save post
        db.add(post)
        db.commit()
        
        # Enrich in background
        background_tasks.add_task(enrich_post_with_links, post)
```

### Best Practices

```python
# 1. Batch extraction
urls_batch = [url for post in posts for url in extract_urls(post.text)]
urls_batch = urls_batch[:100]  # Limit

# 2. Caching –≤ Redis
def cache_crawled_url(url: str, content: str):
    url_hash = hashlib.md5(url.encode()).hexdigest()
    redis_client.setex(
        f"crawl:{url_hash}",
        86400 * 7,  # 1 –Ω–µ–¥–µ–ª—è
        content
    )

# 3. Retry –¥–ª—è 5xx –æ—à–∏–±–æ–∫
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
async def crawl_url_with_retry(url: str):
    # ... crawl logic ...
    pass
```

## ü§ñ Ollama (Local LLM)

### Configuration

```python
class OllamaConfig:
    URL = "http://ollama:11434"
    DEFAULT_MODEL = "llama3.2"
    TIMEOUT = 60  # Seconds (LLM slower)
```

### Use Case: Private Processing

```python
async def anonymize_with_ollama(text: str) -> str:
    """
    –õ–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ –æ–±–ª–∞–∫–æ
    """
    async with httpx.AsyncClient(timeout=OllamaConfig.TIMEOUT) as client:
        response = await client.post(
            f"{OllamaConfig.URL}/api/generate",
            json={
                "model": OllamaConfig.DEFAULT_MODEL,
                "prompt": f"–£–¥–∞–ª–∏ –≤—Å–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞:\n\n{text}",
                "stream": False
            }
        )
        
        response.raise_for_status()
        return response.json()["response"]

# Usage
async def process_sensitive_post(post: Post):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ª–æ–∫–∞–ª—å–Ω–æ
    """
    # 1. Anonymize —á–µ—Ä–µ–∑ Ollama
    anonymized_text = await anonymize_with_ollama(post.text)
    
    # 2. Index anonymized version
    await rag_indexer.index_post(
        post_id=post.id,
        text_override=anonymized_text
    )
```

## üîó n8n & Flowise Integration

### Webhooks –¥–ª—è —Å–æ–±—ã—Ç–∏–π

```python
# parser_service.py
async def notify_webhooks(event: str, data: dict):
    """
    –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–±—ã—Ç–∏–π –≤ n8n/Flowise
    """
    webhook_url = os.getenv(f"WEBHOOK_{event.upper()}")
    
    if not webhook_url:
        return
    
    try:
        async with httpx.AsyncClient() as client:
            await client.post(
                webhook_url,
                json={
                    "event": event,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "data": data
                },
                timeout=10.0
            )
    except Exception as e:
        logger.warning(f"Webhook failed: {e}")

# –°–æ–±—ã—Ç–∏—è
WEBHOOK_EVENTS = [
    "new_post",       # –ù–æ–≤—ã–π –ø–æ—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω
    "post_tagged",    # –ü–æ—Å—Ç –ø–æ–ª—É—á–∏–ª AI-—Ç–µ–≥–∏
    "post_indexed",   # –ü–æ—Å—Ç –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –≤ Qdrant
    "digest_sent"     # –î–∞–π–¥–∂–µ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω
]

# Usage
async def after_post_parsed(post: Post):
    await notify_webhooks("new_post", {
        "post_id": post.id,
        "channel": post.channel.channel_username,
        "text": post.text[:200]
    })
```

### n8n-friendly Endpoints

```python
@app.get("/users/{user_id}/posts/recent")
async def get_recent_posts_for_n8n(
    user_id: int,
    hours: int = 24,
    include_embeddings: bool = False,
    db: Session = Depends(get_db)
):
    """
    Rich payload –¥–ª—è n8n workflows
    """
    posts = db.query(Post).filter(
        Post.user_id == user_id,
        Post.posted_at >= datetime.now(timezone.utc) - timedelta(hours=hours)
    ).all()
    
    return [{
        "id": p.id,
        "text": p.text,
        "enriched_content": p.enriched_content,
        "tags": p.tags,
        "channel": p.channel.channel_username,
        "url": p.url,
        "posted_at": p.posted_at.isoformat(),
        "indexed": get_indexing_status(p.id),
        "embedding": get_embedding(p.id) if include_embeddings else None
    } for p in posts]
```

## ‚úÖ Verification Steps

1. **Qdrant –¥–æ—Å—Ç—É–ø–µ–Ω:**
```bash
curl http://localhost:6333/collections
# {"result": {"collections": [...]}}
```

2. **Redis –¥–æ—Å—Ç—É–ø–µ–Ω:**
```bash
docker exec redis redis-cli PING
# PONG
```

3. **Searxng –¥–æ—Å—Ç—É–ø–µ–Ω:**
```bash
curl -u "$USERNAME:$PASSWORD" \
  "https://searxng.produman.studio/search?q=test&format=json"
# {"results": [...]}
```

4. **Crawl4AI –¥–æ—Å—Ç—É–ø–µ–Ω:**
```bash
curl -X POST http://localhost:11235/crawl \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'
# {"markdown": "...", ...}
```

5. **Ollama –¥–æ—Å—Ç—É–ø–µ–Ω:**
```bash
curl http://localhost:11434/api/tags
# {"models": [...]}
```

## üö® Troubleshooting

### Qdrant –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω

```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker ps | grep qdrant

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏
docker logs qdrant

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API
curl http://localhost:6333/
```

### Crawl4AI timeout

```python
# –£–≤–µ–ª–∏—á—å—Ç–µ timeout
Crawl4AIConfig.TIMEOUT = 60

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ retry
@retry(stop=stop_after_attempt(3))
async def crawl_url(url: str):
    pass
```

## üìö –°–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞

- `03-database.mdc` ‚Äî Redis cache
- `07-rag.mdc` ‚Äî Qdrant, embeddings
- `08-api.mdc` ‚Äî n8n endpoints
