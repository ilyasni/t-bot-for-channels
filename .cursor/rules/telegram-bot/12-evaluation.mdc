---
title: "LLM Evaluation System"
description: "–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ LLM –±–æ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ragas –∏ golden datasets"
tags: ["evaluation", "llm", "quality", "ragas", "testing"]
version: "1.0"
---

# LLM Evaluation System

## üéØ Overview

–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ Telegram –±–æ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º:
- **Ragas** - framework –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RAG —Å–∏—Å—Ç–µ–º
- **Golden Datasets** - —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
- **Custom Metrics** - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Telegram –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- **Prometheus/Grafana** - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

## üèóÔ∏è Architecture

```
evaluation/
‚îú‚îÄ‚îÄ schemas.py              # Pydantic –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ golden_dataset_manager.py  # CRUD –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è datasets
‚îú‚îÄ‚îÄ bot_evaluator.py        # –õ–æ–≥–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏ —á–µ—Ä–µ–∑ Ragas
‚îú‚îÄ‚îÄ evaluation_runner.py    # –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è batch evaluations
‚îú‚îÄ‚îÄ langfuse_integration.py # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Langfuse
‚îú‚îÄ‚îÄ metrics.py             # Prometheus –º–µ—Ç—Ä–∏–∫–∏
‚îî‚îÄ‚îÄ cli.py                 # CLI –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
```

## üìä Data Models

### GoldenDatasetItem

```python
class GoldenDatasetItem(BaseModel):
    item_id: str                    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
    dataset_name: str               # –ù–∞–∑–≤–∞–Ω–∏–µ dataset
    category: str                   # automotive, tech, groups
    input: Dict[str, Any]          # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    query: str                     # –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
    telegram_context: TelegramContext  # –ö–æ–Ω—Ç–µ–∫—Å—Ç Telegram
    expected_output: str           # –û–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç
    retrieved_contexts: List[str]  # Retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
    difficulty: DifficultyLevel    # beginner, intermediate, advanced
    tone: ToneType                # technical, casual, formal
    requires_multi_source: bool    # –¢—Ä–µ–±—É–µ—Ç —Å–∏–Ω—Ç–µ–∑ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
```

### TelegramContext

```python
class TelegramContext(BaseModel):
    user_id: int
    channels: List[str]            # –ü–æ–¥–ø–∏—Å–∞–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã
    groups: List[str]              # –£—á–∞—Å—Ç–∏–µ –≤ –≥—Ä—É–ø–ø–∞—Ö
    context_type: ContextType      # single_channel, multi_channel, group_discussion
    message_history: List[str]     # –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
```

## üßÆ Evaluation Metrics

### Standard Ragas Metrics

```python
# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Ragas
AnswerCorrectness(llm=evaluator_llm)      # –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
Faithfulness(llm=evaluator_llm)           # –í–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
ContextRelevance(llm=evaluator_llm)       # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
FactualCorrectness(llm=evaluator_llm)     # –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
```

### Custom Telegram Metrics

```python
# –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Telegram
ChannelContextAwareness      # –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –∫–∞–Ω–∞–ª–∞
GroupSynthesisQuality        # –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑ –≥—Ä—É–ø–ø
MultiSourceCoherence         # –°–≤—è–∑–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
ToneAppropriateness          # –£–º–µ—Å—Ç–Ω–æ—Å—Ç—å —Ç–æ–Ω–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
```

## üîß Usage Patterns

### 1. Creating Golden Datasets

```python
# –ß–µ—Ä–µ–∑ CLI
python -m evaluation.cli create-dataset --name automotive_qa --file data/automotive_qa.json

# –ß–µ—Ä–µ–∑ API
POST /evaluation/datasets
{
    "name": "automotive_qa",
    "items": [...]
}
```

### 2. Running Evaluations

```python
# –ß–µ—Ä–µ–∑ Telegram –∫–æ–º–∞–Ω–¥—ã (Admin)
/evaluate automotive_qa run_2024_01_15

# –ß–µ—Ä–µ–∑ CLI
python -m evaluation.cli run-evaluation --dataset automotive_qa --run-name test_run

# –ß–µ—Ä–µ–∑ API
POST /evaluation/batch
{
    "dataset_name": "automotive_qa",
    "run_name": "test_run",
    "model_provider": "openrouter",
    "model_name": "gpt-4o-mini"
}
```

### 3. Monitoring Results

```python
# –°—Ç–∞—Ç—É—Å evaluation run
GET /evaluation/status/{run_id}

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
GET /evaluation/results/{run_id}

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ dataset
python -m evaluation.cli dataset-stats --dataset automotive_qa
```

## üéØ Best Practices

### 1. Dataset Creation

```python
# ‚úÖ –•–æ—Ä–æ—à–æ - —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
{
    "item_id": "auto_001",
    "category": "automotive",
    "query": "–ö–∞–∫–∏–µ –Ω–æ–≤—ã–µ —ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª–∏ –≤ 2024?",
    "difficulty": "beginner",
    "tone": "technical",
    "requires_multi_source": False
}

# ‚ùå –ü–ª–æ—Ö–æ - —Å–ª–∏—à–∫–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ
{
    "item_id": "auto_001", 
    "query": "–ß—Ç–æ –¥—É–º–∞–µ—Ç –ò–ª–æ–Ω –ú–∞—Å–∫ –ø—Ä–æ Cybertruck?",
    "difficulty": "expert",
    "requires_multi_source": True
}
```

### 2. Metric Selection

```python
# ‚úÖ –î–ª—è single channel queries
metrics = [
    AnswerCorrectness,
    ChannelContextAwareness,
    ToneAppropriateness
]

# ‚úÖ –î–ª—è multi-source queries  
metrics = [
    AnswerCorrectness,
    MultiSourceCoherence,
    GroupSynthesisQuality
]
```

### 3. Error Handling

```python
# ‚úÖ Graceful degradation
try:
    result = await evaluator.evaluate_item(...)
except RagasEvaluationError as e:
    logger.warning(f"Ragas evaluation failed: {e}")
    return {"overall_score": 0.0, "error": str(e)}
```

## üìà Monitoring & Metrics

### Prometheus Metrics

```python
# Score distributions
evaluation_answer_correctness_histogram
evaluation_faithfulness_histogram
evaluation_context_relevance_histogram

# Run statistics
evaluation_runs_total_counter
evaluation_duration_seconds_histogram
evaluation_items_processed_counter
```

### Grafana Dashboard

- **Evaluation Overview**: –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ runs
- **Score Distributions**: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
- **Performance Trends**: –¢—Ä–µ–Ω–¥—ã –∫–∞—á–µ—Å—Ç–≤–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏
- **Error Analysis**: –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∏ failures

## üîÑ Integration Points

### 1. Langfuse Integration

```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ Langfuse
await langfuse_client.log_evaluation_result(
    trace_id=langfuse_trace_id,
    scores=evaluation_scores,
    metadata=evaluation_metadata
)
```

### 2. Admin Commands

```python
# Telegram admin –∫–æ–º–∞–Ω–¥—ã
/evaluate <dataset> <run_name> [model_provider] [model_name]
/evaluate_status [run_name]
/evaluate_results <run_name>
/evaluate_datasets
```

### 3. FastAPI Endpoints

```python
# REST API –¥–ª—è integration
POST /evaluation/batch
GET /evaluation/status/{run_id}
GET /evaluation/results/{run_id}
GET /evaluation/datasets
```

## üß™ Testing Strategy

### Unit Tests

```python
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ö–µ–º
def test_golden_dataset_item_validation():
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ Pydantic –º–æ–¥–µ–ª–µ–π

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ manager
async def test_golden_dataset_manager_crud():
    # CRUD –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –º–æ–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º DB

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ evaluator
async def test_bot_evaluator_with_mocked_ragas():
    # –ú–æ–∫–∏—Ä–æ–≤–∞–Ω–∏–µ Ragas –¥–ª—è unit —Ç–µ—Å—Ç–æ–≤
```

### Integration Tests

```python
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ pipeline
async def test_evaluation_pipeline():
    # –°–æ–∑–¥–∞–Ω–∏–µ dataset ‚Üí evaluation ‚Üí –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API endpoints
def test_evaluation_api_endpoints():
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ FastAPI endpoints
```

## üö® Common Pitfalls

### 1. Ragas Dependency

```python
# ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ragas
if not RAGAS_AVAILABLE:
    logger.warning("Ragas not available, using fallback evaluation")
    return fallback_evaluation()

# ‚ùå –ë–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏
result = ragas_metric.score(...)  # –ú–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å
```

### 2. Timezone Handling

```python
# ‚úÖ Timezone-aware timestamps
created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

# ‚ùå Naive datetime
created_at: datetime = Field(default_factory=datetime.now)
```

### 3. Database Transactions

```python
# ‚úÖ Proper transaction handling
async with conn.transaction():
    await conn.execute("INSERT INTO evaluation_run ...")
    await conn.execute("INSERT INTO evaluation_result ...")

# ‚ùå –ë–µ–∑ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
await conn.execute("INSERT INTO evaluation_run ...")
await conn.execute("INSERT INTO evaluation_result ...")  # –ú–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å
```

## üìö Related Rules

- `03-database.mdc` ‚Äî PostgreSQL —Å—Ö–µ–º—ã –¥–ª—è evaluation
- `08-api.mdc` ‚Äî FastAPI endpoints –¥–ª—è evaluation
- `09-external.mdc` ‚Äî –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Ragas, Langfuse
- `04-development.mdc` ‚Äî –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ evaluation —Å–∏—Å—Ç–µ–º—ã

## ‚úÖ Verification Checklist

–ü–æ—Å–ª–µ —Ä–∞–±–æ—Ç—ã —Å evaluation —Å–∏—Å—Ç–µ–º–æ–π:

1. **Dataset validation:**
   - [ ] –í—Å–µ required –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω—ã
   - [ ] Telegram context –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π
   - [ ] Difficulty –∏ tone —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∫–æ–Ω—Ç–µ–Ω—Ç—É

2. **Evaluation setup:**
   - [ ] Ragas metrics –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
   - [ ] LLM provider –Ω–∞—Å—Ç—Ä–æ–µ–Ω
   - [ ] Prometheus –º–µ—Ç—Ä–∏–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç

3. **Results validation:**
   - [ ] Scores –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0.0-1.0
   - [ ] Langfuse integration —Ä–∞–±–æ—Ç–∞–µ—Ç
   - [ ] Database –∑–∞–ø–∏—Å–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ

4. **Error handling:**
   - [ ] Graceful degradation –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö Ragas
   - [ ] Proper logging –æ—à–∏–±–æ–∫
   - [ ] Fallback evaluation –¥–æ—Å—Ç—É–ø–µ–Ω