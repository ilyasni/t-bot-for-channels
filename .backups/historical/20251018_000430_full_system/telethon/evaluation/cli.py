#!/usr/bin/env python3
"""
CLI –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è evaluation system

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- –°–æ–∑–¥–∞–Ω–∏–µ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ golden datasets
- –ó–∞–ø—É—Å–∫ batch evaluation
- –ü—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ evaluation
- –≠–∫—Å–ø–æ—Ä—Ç/–∏–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python cli.py dataset list
    python cli.py dataset create --name my_dataset --file dataset.json
    python cli.py evaluation run --dataset my_dataset --model gpt-4
    python cli.py evaluation results --run-id abc123
"""

import argparse
import asyncio
import json
import logging
import os
import sys
from pathlib import Path
from typing import Dict, Any, Optional

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent.parent))

try:
    from evaluation.golden_dataset_manager import GoldenDatasetManager
    from evaluation.schemas import GoldenDatasetCreate, GoldenDatasetItem
except ImportError:
    # Fallback –¥–ª—è —Å–ª—É—á–∞–µ–≤ –∫–æ–≥–¥–∞ –º–æ–¥—É–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
    from golden_dataset_manager import GoldenDatasetManager
    from schemas import GoldenDatasetCreate, GoldenDatasetItem

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EvaluationCLI:
    """CLI –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è evaluation system"""
    
    def __init__(self):
        # –ó–∞–º–µ–Ω—è–µ–º supabase-db –Ω–∞ localhost –¥–ª—è CLI
        database_url = os.getenv(
            "TELEGRAM_DATABASE_URL", 
            "postgresql://postgres:password@localhost:5432/postgres"
        )
        # –ó–∞–º–µ–Ω—è–µ–º –∏–º—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –Ω–∞ localhost
        if "supabase-db:5432" in database_url:
            database_url = database_url.replace("supabase-db:5432", "localhost:5432")
        
        self.database_url = database_url
        self.dataset_manager = GoldenDatasetManager(self.database_url)
    
    async def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            await self.dataset_manager.initialize()
            logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
            raise
    
    async def list_datasets(self):
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö golden datasets"""
        try:
            datasets = await self.dataset_manager.list_datasets()
            if not datasets:
                print("üì≠ Golden datasets –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return
            
            print("üìä Golden Datasets:")
            for dataset in datasets:
                print(f"  ‚Ä¢ {dataset['name']} (v{dataset['version']}) - {dataset['item_count']} items")
                print(f"    –û–ø–∏—Å–∞–Ω–∏–µ: {dataset['description']}")
                print(f"    –°–æ–∑–¥–∞–Ω: {dataset['created_at']}")
                print()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ datasets: {e}")
    
    async def create_dataset_from_file(self, name: str, file_path: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ dataset –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if 'items' not in data:
                raise ValueError("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–ª–µ 'items'")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ dataset
            dataset_create = GoldenDatasetCreate(
                name=name,
                description=data.get('description', ''),
                version=data.get('version', '1.0.0')
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ items
            items = []
            for item_data in data['items']:
                item = GoldenDatasetItem(**item_data)
                items.append(item)
            
            dataset_id = await self.dataset_manager.create_dataset(dataset_create)
            await self.dataset_manager.add_items(dataset_id, items)
            
            print(f"‚úÖ Dataset '{name}' —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ (ID: {dataset_id})")
            print(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ {len(items)} items")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è dataset: {e}")
    
    async def export_dataset(self, name: str, output_file: str):
        """–≠–∫—Å–ø–æ—Ä—Ç dataset –≤ JSON —Ñ–∞–π–ª"""
        try:
            dataset = await self.dataset_manager.get_dataset_by_name(name)
            if not dataset:
                print(f"‚ùå Dataset '{name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return
            
            items = await self.dataset_manager.get_dataset_items(dataset['id'])
            
            export_data = {
                "dataset_name": dataset['name'],
                "description": dataset['description'],
                "version": dataset['version'],
                "created_at": dataset['created_at'].isoformat(),
                "items": [
                    {
                        "question": item['question'],
                        "expected_answer": item['expected_answer'],
                        "contexts": item['contexts'],
                        "difficulty": item['difficulty'],
                        "category": item['category'],
                        "tags": item['tags']
                    }
                    for item in items
                ]
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ Dataset '{name}' —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ {output_file}")
            print(f"   –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(items)} items")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ dataset: {e}")
    
    async def run_evaluation(self, dataset_name: str, model_name: str = "gpt-4"):
        """–ó–∞–ø—É—Å–∫ evaluation –Ω–∞ dataset"""
        try:
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å EvaluationRunner
            print(f"üöÄ –ó–∞–ø—É—Å–∫ evaluation –Ω–∞ dataset '{dataset_name}' —Å –º–æ–¥–µ–ª—å—é '{model_name}'")
            print("‚ö†Ô∏è  –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å EvaluationRunner –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–∑–∂–µ")
            
            # –ü–æ–∫–∞ —á—Ç–æ –ø—Ä–æ—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ dataset
            dataset = await self.dataset_manager.get_dataset_by_name(dataset_name)
            if not dataset:
                print(f"‚ùå Dataset '{dataset_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return
            
            items = await self.dataset_manager.get_dataset_items(dataset['id'])
            print(f"üìä Dataset —Å–æ–¥–µ—Ä–∂–∏—Ç {len(items)} items –¥–ª—è evaluation")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ evaluation: {e}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI"""
    parser = argparse.ArgumentParser(description="Evaluation System CLI")
    subparsers = parser.add_subparsers(dest='command', help='–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã')
    
    # Dataset commands
    dataset_parser = subparsers.add_parser('dataset', help='–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ golden datasets')
    dataset_subparsers = dataset_parser.add_subparsers(dest='dataset_action')
    
    dataset_subparsers.add_parser('list', help='–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö datasets')
    
    create_parser = dataset_subparsers.add_parser('create', help='–°–æ–∑–¥–∞—Ç—å dataset –∏–∑ —Ñ–∞–π–ª–∞')
    create_parser.add_argument('--name', required=True, help='–ò–º—è dataset')
    create_parser.add_argument('--file', required=True, help='–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É')
    
    export_parser = dataset_subparsers.add_parser('export', help='–≠–∫—Å–ø–æ—Ä—Ç dataset –≤ —Ñ–∞–π–ª')
    export_parser.add_argument('--name', required=True, help='–ò–º—è dataset')
    export_parser.add_argument('--output', required=True, help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É')
    
    # Evaluation commands
    eval_parser = subparsers.add_parser('evaluation', help='–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ evaluation')
    eval_subparsers = eval_parser.add_subparsers(dest='eval_action')
    
    run_parser = eval_subparsers.add_parser('run', help='–ó–∞–ø—É—Å—Ç–∏—Ç—å evaluation')
    run_parser.add_argument('--dataset', required=True, help='–ò–º—è dataset')
    run_parser.add_argument('--model', default='gpt-4', help='–ú–æ–¥–µ–ª—å –¥–ª—è evaluation')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CLI
    cli = EvaluationCLI()
    
    async def run_command():
        await cli.init_database()
        
        if args.command == 'dataset':
            if args.dataset_action == 'list':
                await cli.list_datasets()
            elif args.dataset_action == 'create':
                await cli.create_dataset_from_file(args.name, args.file)
            elif args.dataset_action == 'export':
                await cli.export_dataset(args.name, args.output)
            else:
                dataset_parser.print_help()
        
        elif args.command == 'evaluation':
            if args.eval_action == 'run':
                await cli.run_evaluation(args.dataset, args.model)
            else:
                eval_parser.print_help()
    
    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∫–æ–º–∞–Ω–¥—ã
    try:
        asyncio.run(run_command())
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()