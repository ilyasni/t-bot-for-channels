"""
Query Expander
–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ Neo4j tag relationships

Best practices:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å tag co-occurrence –∏–∑ –≥—Ä–∞—Ñ–∞
- –ò–∑–±–µ–≥–∞—Ç—å over-expansion (max 3-5 —Ç–µ—Ä–º–∏–Ω–æ–≤)
- Graceful degradation (—Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ Neo4j)
"""
import logging
import re
from typing import List, Set, Optional
import os

# –ò–º–ø–æ—Ä—Ç—ã
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from graph.neo4j_client import neo4j_client

logger = logging.getLogger(__name__)


class QueryExpander:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ Neo4j tag relationships
    
    Example:
        ```python
        expander = QueryExpander()
        
        original = "AI –Ω–æ–≤–æ—Å—Ç–∏"
        expanded = await expander.expand_query(original)
        # ‚Üí "AI –Ω–æ–≤–æ—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ ChatGPT"
        ```
    
    Best practice: Query expansion —É–ª—É—á—à–∞–µ—Ç recall –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –ø–æ–∏—Å–∫–µ
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è query expander"""
        self.enabled = neo4j_client.enabled
        self.max_expansions = int(os.getenv("QUERY_EXPANSION_MAX_TERMS", "3"))
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–Ω–µ —Ä–∞—Å—à–∏—Ä—è–µ–º –∏—Ö)
        self.stop_words = {
            '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–∞–∫', '–ø–æ—á–µ–º—É', '–∫–∞–∫–∏–µ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è',
            '–ø—Ä–æ', '–¥–ª—è', '–±—ã–ª', '–±—ã–ª–∏', '–±—ã–ª–æ', '–µ—Å—Ç—å', '—ç—Ç–æ', '—ç—Ç–∏', '—ç—Ç–æ—Ç',
            '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '—Ä–∞—Å—Å–∫–∞–∂–∏', '—Å–¥–µ–ª–∞–π', '–ø–æ–∫–∞–∂–∏', '–¥–∞–π',
            '–ø–∏—Å–∞–ª–∏', '–≥–æ–≤–æ—Ä–∏–ª–∏', '–ø—Ä–æ–∏–∑–æ—à–ª–æ', '—Å–ª—É—á–∏–ª–æ—Å—å', '–∑–∞', '–Ω–∞', '–≤', '—Å',
            '–∏', '–∏–ª–∏', '–Ω–æ', '–∞', '—Ç–æ'
        }
        
        logger.info(f"‚úÖ QueryExpander initialized (enabled: {self.enabled}, max_expansions: {self.max_expansions})")
    
    async def expand_query(
        self,
        query: str,
        user_id: Optional[int] = None,
        max_terms: Optional[int] = None
    ) -> str:
        """
        –†–∞—Å—à–∏—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ tag relationships –∏–∑ –≥—Ä–∞—Ñ–∞
        
        Args:
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –±—É–¥—É—â–µ–º)
            max_terms: –ú–∞–∫—Å–∏–º—É–º —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è (default: –∏–∑ config)
        
        Returns:
            –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π (–µ—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ)
        
        Algorithm:
            1. –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –Ω–∞–π—Ç–∏ related tags –≤ Neo4j
            3. –î–æ–±–∞–≤–∏—Ç—å top N related tags –∫ –∑–∞–ø—Ä–æ—Å—É
            4. Return expanded query
        """
        if not self.enabled:
            return query
        
        if max_terms is None:
            max_terms = self.max_expansions
        
        try:
            # 1. –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self._extract_keywords(query)
            
            if not keywords:
                logger.debug("No keywords found for expansion")
                return query
            
            logger.debug(f"üîç Keywords extracted: {keywords}")
            
            # 2. –ù–∞–π—Ç–∏ related tags –¥–ª—è –∫–∞–∂–¥–æ–≥–æ keyword
            expanded_terms: Set[str] = set()
            
            for keyword in keywords[:2]:  # Limit to first 2 keywords (avoid over-expansion)
                try:
                    # –ó–∞–ø—Ä–æ—Å–∏—Ç—å related tags –∏–∑ Neo4j
                    related = await neo4j_client.get_tag_relationships(
                        tag_name=keyword,
                        limit=max_terms
                    )
                    
                    if related:
                        # –î–æ–±–∞–≤–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è related tags
                        related_names = [r.get('tag') for r in related if r.get('tag')]
                        expanded_terms.update(related_names[:max_terms])
                        logger.debug(f"   '{keyword}' ‚Üí {related_names[:max_terms]}")
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to get related tags for '{keyword}': {e}")
            
            # 3. –°–æ–±—Ä–∞—Ç—å expanded query
            if expanded_terms:
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –µ—Å—Ç—å –≤ –∑–∞–ø—Ä–æ—Å–µ
                query_lower = query.lower()
                new_terms = [
                    term for term in expanded_terms
                    if term.lower() not in query_lower
                ]
                
                if new_terms:
                    # Limit to max_terms
                    new_terms = new_terms[:max_terms]
                    expanded_query = f"{query} {' '.join(new_terms)}"
                    
                    logger.info(f"‚ú® Query expanded: '{query}' ‚Üí '{expanded_query}'")
                    return expanded_query
            
            # –ù–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
            logger.debug(f"No expansion for query: '{query}'")
            return query
            
        except Exception as e:
            logger.error(f"‚ùå Query expansion failed: {e}")
            return query  # Fallback to original
    
    def _extract_keywords(self, query: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        
        Args:
            query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, min length=3)
        
        Best practice: Simple regex-based extraction
        """
        # –û—á–∏—Å—Ç–∏—Ç—å –æ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        query_clean = query.lower()
        query_clean = re.sub(r'[^\w\s]', ' ', query_clean)
        
        # –†–∞–∑–±–∏—Ç—å –Ω–∞ —Å–ª–æ–≤–∞
        words = query_clean.split()
        
        # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å:
        # - –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        # - –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (< 3 —Å–∏–º–≤–æ–ª–∞)
        # - –î—É–±–ª–∏–∫–∞—Ç—ã
        keywords = []
        seen = set()
        
        for word in words:
            if (
                len(word) >= 3 and
                word not in self.stop_words and
                word not in seen
            ):
                keywords.append(word)
                seen.add(word)
        
        return keywords
    
    def add_stop_word(self, word: str):
        """
        –î–æ–±–∞–≤–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–æ (–¥–ª—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏)
        
        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ stop list
        """
        self.stop_words.add(word.lower())
        logger.debug(f"Added stop word: {word}")
    
    def remove_stop_word(self, word: str):
        """
        –£–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–æ
        
        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ stop list
        """
        self.stop_words.discard(word.lower())
        logger.debug(f"Removed stop word: {word}")


# Singleton instance
query_expander = QueryExpander()

