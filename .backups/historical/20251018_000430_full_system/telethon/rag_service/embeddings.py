"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
1. EmbeddingsGigaR —á–µ—Ä–µ–∑ gpt2giga-proxy (–æ—Å–Ω–æ–≤–Ω–æ–π)
2. sentence-transformers (fallback)
"""
import logging
import httpx
import tiktoken
from typing import List, Optional, Tuple
import config
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ observability
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Observability
try:
    from observability.langfuse_client import langfuse_client
    from observability.metrics import rag_embeddings_duration_seconds, rag_query_errors_total
except ImportError:
    # Graceful degradation –µ—Å–ª–∏ observability –º–æ–¥—É–ª—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    logger = logging.getLogger(__name__)
    logger.warning("‚ö†Ô∏è Observability modules not available")
    langfuse_client = None
    rag_embeddings_duration_seconds = None
    rag_query_errors_total = None

logger = logging.getLogger(__name__)


class EmbeddingsService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ embeddings"""
        self.gigachat_url = f"{config.GIGACHAT_PROXY_URL}/v1/embeddings"
        self.gigachat_enabled = config.GIGACHAT_ENABLED
        
        # Tokenizer –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ (cl100k_base - –¥–ª—è OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π)
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å tokenizer: {e}")
            self.tokenizer = None
        
        # Sentence-transformers –¥–ª—è fallback (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
        self.sentence_transformer_model = None
        self.fallback_model_name = config.EMBEDDING_MODEL
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ (–æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self.gigachat_vector_size = None
        self.fallback_vector_size = 768  # –ò–∑–≤–µ—Å—Ç–Ω–æ –¥–ª—è sentence-transformers
        
        logger.info(f"‚úÖ Embeddings —Å–µ—Ä–≤–∏—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"   GigaChat proxy: {self.gigachat_url} (enabled={self.gigachat_enabled})")
        logger.info(f"   Fallback model: {self.fallback_model_name}")
    
    def count_tokens(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        else:
            # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞
            return len(text) // 4
    
    def chunk_text(
        self,
        text: str,
        max_tokens: int,
        overlap_tokens: int
    ) -> List[Tuple[str, int, int]]:
        """
        –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ chunks —Å overlap
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ chunk
            overlap_tokens: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è overlap
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (chunk_text, start_pos, end_pos)
        """
        if not text or not text.strip():
            return []
        
        total_tokens = self.count_tokens(text)
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—á–µ max_tokens, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ —Ü–µ–ª–∏–∫–æ–º
        if total_tokens <= max_tokens:
            return [(text, 0, len(text))]
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ chunks
        chunks = []
        
        if self.tokenizer:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º tokenizer –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
            tokens = self.tokenizer.encode(text)
            
            start = 0
            while start < len(tokens):
                end = min(start + max_tokens, len(tokens))
                chunk_tokens = tokens[start:end]
                chunk_text = self.tokenizer.decode(chunk_tokens)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
                char_start = len(self.tokenizer.decode(tokens[:start]))
                char_end = len(self.tokenizer.decode(tokens[:end]))
                
                chunks.append((chunk_text, char_start, char_end))
                
                # –î–≤–∏–≥–∞–µ–º—Å—è –≤–ø–µ—Ä–µ–¥ —Å —É—á–µ—Ç–æ–º overlap
                start = end - overlap_tokens if end < len(tokens) else end
        else:
            # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º (fallback)
            chars_per_chunk = max_tokens * 4  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
            overlap_chars = overlap_tokens * 4
            
            start = 0
            while start < len(text):
                end = min(start + chars_per_chunk, len(text))
                chunk_text = text[start:end]
                chunks.append((chunk_text, start, end))
                
                start = end - overlap_chars if end < len(text) else end
        
        logger.debug(f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} chunks (max_tokens={max_tokens}, overlap={overlap_tokens})")
        return chunks
    
    async def generate_embedding_gigachat(self, text: str) -> Optional[List[float]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding —á–µ—Ä–µ–∑ GigaChat (gpt2giga-proxy)
        –° rate limiting (1 concurrent request) –∏ exponential backoff retry
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è embeddings
            
        Returns:
            –í–µ–∫—Ç–æ—Ä embeddings –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not self.gigachat_enabled:
            return None
        
        # –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è rate limiting –∏ retry
        from rate_limiter import gigachat_rate_limiter
        from tenacity import (
            retry,
            stop_after_attempt,
            wait_exponential,
            retry_if_exception_type,
            RetryError
        )
        
        # Prometheus metrics timing
        if rag_embeddings_duration_seconds:
            timer = rag_embeddings_duration_seconds.labels(provider='gigachat').time()
            timer.__enter__()
        else:
            timer = None
        
        try:
            # Langfuse tracing
            trace_ctx = langfuse_client.trace_context(
                "embedding_generation",
                metadata={"provider": "gigachat", "text_length": len(text)}
            ) if langfuse_client else None
            
            trace = None
            if trace_ctx:
                trace = trace_ctx.__enter__()
            
            # –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è —Å retry –¥–ª—è GigaChat API
            @retry(
                retry=retry_if_exception_type(httpx.HTTPStatusError),
                stop=stop_after_attempt(3),
                wait=wait_exponential(multiplier=1, min=2, max=10),
                reraise=True
            )
            async def _generate_with_retry():
                # –ö–†–ò–¢–ò–ß–ù–û: Rate limiter –¥–ª—è 1 concurrent request
                async with gigachat_rate_limiter:
                    logger.debug("üîí Acquired rate limit slot for GigaChat")
                    
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        response = await client.post(
                            self.gigachat_url,
                            json={
                                "input": text,
                                "model": "EmbeddingsGigaR"  # –ú–æ–¥–µ–ª—å –¥–ª—è embeddings –≤ GigaChat
                            }
                        )
                        
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ 429 Rate Limit
                        if response.status_code == 429:
                            logger.warning(f"‚ö†Ô∏è GigaChat 429 Rate Limit, retry...")
                            response.raise_for_status()  # Trigger retry
                        
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫
                        if response.status_code != 200:
                            logger.error(f"‚ùå GigaChat error {response.status_code}: {response.text[:200]}")
                            response.raise_for_status()  # Trigger retry for 5xx errors
                        
                        return response.json()
            
            # –í—ã–∑—ã–≤–∞–µ–º —Å retry
            result = await _generate_with_retry()
            embedding = result["data"][0]["embedding"]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ
            if self.gigachat_vector_size is None:
                self.gigachat_vector_size = len(embedding)
                logger.info(f"‚úÖ GigaChat vector size: {self.gigachat_vector_size}")
            
            # Update trace with result
            if trace:
                trace.update(metadata={"embedding_dim": len(embedding)})
            
            return embedding
            
        except RetryError as e:
            # –í—Å–µ retry –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
            logger.error(f"‚ùå GigaChat failed after all retries: {e}")
            if rag_query_errors_total:
                rag_query_errors_total.labels(error_type='gigachat_retry_exhausted').inc()
            return None
        except httpx.TimeoutException as e:
            logger.error(f"‚ùå GigaChat timeout: {e}")
            if rag_query_errors_total:
                rag_query_errors_total.labels(error_type='gigachat_timeout').inc()
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ GigaChat embeddings: {e}")
            if rag_query_errors_total:
                rag_query_errors_total.labels(error_type='embedding_failed').inc()
            return None
        finally:
            if timer:
                timer.__exit__(None, None, None)
            if trace_ctx:
                trace_ctx.__exit__(None, None, None)
    
    def _load_sentence_transformer(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ sentence-transformers –º–æ–¥–µ–ª–∏"""
        if self.sentence_transformer_model is None:
            try:
                from sentence_transformers import SentenceTransformer
                logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ fallback –º–æ–¥–µ–ª–∏: {self.fallback_model_name}")
                self.sentence_transformer_model = SentenceTransformer(self.fallback_model_name)
                logger.info(f"‚úÖ Fallback –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except ImportError:
                logger.warning("‚ö†Ô∏è sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. Fallback –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
                logger.warning("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers torch")
                return
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ fallback –º–æ–¥–µ–ª–∏: {e}")
                return
    
    async def generate_embedding_fallback(self, text: str) -> Optional[List[float]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding —á–µ—Ä–µ–∑ sentence-transformers (fallback)
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è embeddings
            
        Returns:
            –í–µ–∫—Ç–æ—Ä embeddings –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            self._load_sentence_transformer()
            
            if self.sentence_transformer_model is None:
                return None
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding
            embedding = self.sentence_transformer_model.encode(
                text,
                convert_to_numpy=True,
                show_progress_bar=False
            )
            
            return embedding.tolist()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ fallback embeddings: {e}")
            return None
    
    async def generate_embedding(self, text: str) -> Optional[Tuple[List[float], str]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è embeddings
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–≤–µ–∫—Ç–æ—Ä embeddings, –ø—Ä–æ–≤–∞–π–¥–µ—Ä) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not text or not text.strip():
            logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è embeddings")
            return None
        
        # –ü—Ä–æ–±—É–µ–º GigaChat
        if self.gigachat_enabled:
            embedding = await self.generate_embedding_gigachat(text)
            if embedding:
                return embedding, "gigachat"
            else:
                logger.warning("‚ö†Ô∏è GigaChat embeddings –Ω–µ —É–¥–∞–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        
        # Fallback –Ω–∞ sentence-transformers
        embedding = await self.generate_embedding_fallback(text)
        if embedding:
            return embedding, "sentence-transformers"
        
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å embeddings –Ω–∏ –æ–¥–Ω–∏–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º")
        return None
    
    async def generate_embeddings_batch(
        self,
        texts: List[str]
    ) -> List[Optional[Tuple[List[float], str]]]:
        """
        Batch –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ embeddings (–∏–ª–∏ None –¥–ª—è –æ—à–∏–±–æ–∫)
        """
        results = []
        
        # TODO: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å batch processing –¥–ª—è GigaChat API
        # –ü–æ–∫–∞ –¥–µ–ª–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
        for text in texts:
            result = await self.generate_embedding(text)
            results.append(result)
        
        return results
    
    def get_chunking_params(self, provider: str = "gigachat") -> Tuple[int, int]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã chunking –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        
        Args:
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä embeddings (gigachat –∏–ª–∏ sentence-transformers)
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (max_tokens, overlap_tokens)
        """
        if provider == "gigachat":
            return (
                config.EMBEDDING_MAX_TOKENS_GIGACHAT,
                config.EMBEDDING_OVERLAP_TOKENS_GIGACHAT
            )
        else:
            return (
                config.EMBEDDING_MAX_TOKENS_FALLBACK,
                config.EMBEDDING_OVERLAP_TOKENS_FALLBACK
            )


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
embeddings_service = EmbeddingsService()

