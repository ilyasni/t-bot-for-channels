"""
Group Digest Generator
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤ –¥–∏–∞–ª–æ–≥–æ–≤ –≤ Telegram –≥—Ä—É–ø–ø–∞—Ö —á–µ—Ä–µ–∑ n8n multi-agent workflows
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ n8n, —Ç–∞–∫ –∏ –ø—Ä—è–º—É—é LangChain –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é
"""
import logging
import os
import httpx
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta, timezone
from telethon.tl.types import Message
from dotenv import load_dotenv
import telegram_formatter

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GroupDigestGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤ –¥–ª—è Telegram –≥—Ä—É–ø–ø —á–µ—Ä–µ–∑ n8n workflows –∏–ª–∏ LangChain"""
    
    def __init__(self):
        # Feature flags
        self.use_langchain_direct = os.getenv("USE_LANGCHAIN_DIRECT", "false").lower() == "true"
        self.use_v2_pipeline = os.getenv("USE_DIGEST_V2", "true").lower() == "true"
        
        if self.use_langchain_direct:
            # LangChain Direct Integration
            logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LangChain Direct Integration")
            try:
                from langchain_agents.orchestrator import DigestOrchestrator
                self.orchestrator = DigestOrchestrator()
                logger.info("‚úÖ LangChain Orchestrator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except ImportError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ LangChain: {e}")
                logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ n8n fallback")
                self.use_langchain_direct = False
        
        if not self.use_langchain_direct:
            # n8n webhook URLs (fallback)
            self.n8n_digest_webhook = os.getenv(
                "N8N_GROUP_DIGEST_WEBHOOK", 
                "http://n8n:5678/webhook/group-digest"
            )
            self.n8n_digest_webhook_v2 = os.getenv(
                "N8N_GROUP_DIGEST_WEBHOOK_V2",
                "http://n8n:5678/webhook/group-digest-v2"
            )
            self.n8n_mention_webhook = os.getenv(
                "N8N_MENTION_ANALYZER_WEBHOOK",
                "http://n8n:5678/webhook/mention-analyzer"
            )
            
            # Timeouts
            self.digest_timeout = float(os.getenv("N8N_DIGEST_TIMEOUT", "120"))
            self.digest_timeout_v2 = float(os.getenv("N8N_DIGEST_TIMEOUT_V2", "180"))
            self.mention_timeout = float(os.getenv("N8N_MENTION_TIMEOUT", "60"))
            
            logger.info("‚úÖ GroupDigestGenerator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å n8n")
            logger.info(f"   V1 Webhook: {self.n8n_digest_webhook}")
            logger.info(f"   V2 Webhook: {self.n8n_digest_webhook_v2}")
            logger.info(f"   Use V2 Pipeline: {self.use_v2_pipeline}")
            logger.info(f"   Mention webhook: {self.n8n_mention_webhook}")
    
    async def generate_digest(
        self, 
        user_id: int, 
        group_id: int, 
        messages: List[Message],
        hours: int = 24
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–π–¥–∂–µ—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ —á–µ—Ä–µ–∑ n8n multi-agent workflow –∏–ª–∏ LangChain
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            group_id: ID –≥—Ä—É–ø–ø—ã
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π Telethon
            hours: –ü–µ—Ä–∏–æ–¥ –≤ —á–∞—Å–∞—Ö
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞
        """
        try:
            logger.info(f"ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞ –¥–ª—è –≥—Ä—É–ø–ø—ã {group_id} (user {user_id})")
            logger.info(f"   –°–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}, –ø–µ—Ä–∏–æ–¥: {hours}—á")
            logger.info(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {'LangChain Direct' if self.use_langchain_direct else 'n8n'}")
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π
            max_messages = int(os.getenv("DIGEST_MAX_MESSAGES", "200"))
            limited_messages = messages[:max_messages]
            
            # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if self.use_langchain_direct:
                return await self._generate_with_langchain(user_id, group_id, limited_messages, hours)
            else:
                return await self._generate_with_n8n(user_id, group_id, limited_messages, hours)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞: {e}")
            raise
    
    async def _generate_with_langchain(
        self, 
        user_id: int, 
        group_id: int, 
        messages: List[Message],
        hours: int
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞ —á–µ—Ä–µ–∑ LangChain Direct Integration
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            group_id: ID –≥—Ä—É–ø–ø—ã
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π Telethon
            hours: –ü–µ—Ä–∏–æ–¥ –≤ —á–∞—Å–∞—Ö
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞
        """
        try:
            logger.info("üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ LangChain Direct Integration")
            
            # –í—ã–∑–æ–≤ LangChain Orchestrator
            result = await self.orchestrator.generate_digest(
                messages=messages,
                hours=hours,
                user_id=user_id,
                group_id=group_id
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if not result.get("success", False):
                raise Exception(f"LangChain generation failed: {result.get('error', 'Unknown error')}")
            
            # –ü–æ–ª—É—á–∞–µ–º Pydantic –æ–±—ä–µ–∫—Ç –¥–∞–π–¥–∂–µ—Å—Ç–∞
            digest_obj = result["digest"]
            
            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
            adapted_result = {
                "html_digest": digest_obj.html_digest,
                "topics": self._extract_topics_from_langchain_result(digest_obj),
                "speakers_summary": self._extract_speakers_from_langchain_result(digest_obj),
                "overall_summary": digest_obj.sections.summary,
                "message_count": len(messages),
                "period": f"{hours}—á",
                "detail_level": digest_obj.metadata.detail_level,
                "dialogue_type": digest_obj.metadata.dialogue_type,
                "generation_method": "langchain_direct",
                "generation_metadata": {
                    "execution_time": result.get("execution_time", 0),
                    "agents_status": result.get("agents_status", [])
                },
                "agent_results": result.get("agent_results", {}),
                "agent_statistics": {
                    "agents_executed": len(result.get("agents_status", [])),
                    "successful_agents": len([s for s in result.get("agents_status", []) if s.status == "success"])
                }
            }
            
            # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç LangChain
            logger.info(f"üìä LangChain —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
            logger.info(f"   HTML –¥–∞–π–¥–∂–µ—Å—Ç: {len(digest_obj.html_digest)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"   –¢–µ–º—ã: {len(adapted_result['topics'])}")
            logger.info(f"   –°–ø–∏–∫–µ—Ä—ã: {len(adapted_result['speakers_summary'])}")
            logger.info(f"   –†–µ–∑—é–º–µ: {len(adapted_result['overall_summary'])} —Å–∏–º–≤–æ–ª–æ–≤")
            
            logger.info("‚úÖ LangChain –¥–∞–π–¥–∂–µ—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            logger.info(f"   –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è: {adapted_result['detail_level']}")
            logger.info(f"   –¢–∏–ø –¥–∏–∞–ª–æ–≥–∞: {adapted_result['dialogue_type']}")
            logger.info(f"   –ê–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {adapted_result['agent_statistics'].get('agents_executed', 0)}")
            
            return adapted_result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LangChain –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            raise
    
    async def _generate_with_n8n(
        self, 
        user_id: int, 
        group_id: int, 
        messages: List[Message],
        hours: int
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞ —á–µ—Ä–µ–∑ n8n workflows (fallback)
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            group_id: ID –≥—Ä—É–ø–ø—ã
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π Telethon
            hours: –ü–µ—Ä–∏–æ–¥ –≤ —á–∞—Å–∞—Ö
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞
        """
        try:
            logger.info("üì° –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ n8n workflows")
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è n8n
            formatted_messages = []
            for msg in messages:
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ username
                username = "Unknown"
                if hasattr(msg, 'sender') and msg.sender:
                    if hasattr(msg.sender, 'username') and msg.sender.username:
                        username = msg.sender.username
                    elif hasattr(msg.sender, 'first_name') and msg.sender.first_name:
                        username = msg.sender.first_name
                    
                    logger.debug(f"Message sender: {username}")
                else:
                    logger.warning(f"Message has no sender information!")
                
                formatted_messages.append({
                    "username": username,
                    "text": msg.text or "",
                    "date": msg.date.isoformat() if hasattr(msg.date, 'isoformat') else str(msg.date)
                })
            
            # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ usernames —á—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º
            unique_usernames = set(m['username'] for m in formatted_messages)
            logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ n8n {len(formatted_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç {len(unique_usernames)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {', '.join(unique_usernames)}")
            
            # –í—ã–∑—ã–≤–∞–µ–º n8n workflow (V1 –∏–ª–∏ V2)
            webhook_url = self.n8n_digest_webhook_v2 if self.use_v2_pipeline else self.n8n_digest_webhook
            timeout = self.digest_timeout_v2 if self.use_v2_pipeline else self.digest_timeout
            
            async with httpx.AsyncClient(timeout=timeout) as client:
                logger.info(f"üì° –í—ã–∑–æ–≤ n8n workflow: {webhook_url}")
                logger.info(f"   Pipeline: {'V2 Sequential' if self.use_v2_pipeline else 'V1 Parallel'}")
                
                response = await client.post(
                    webhook_url,
                    json={
                        "messages": formatted_messages,
                        "user_id": user_id,
                        "group_id": group_id,
                        "hours": hours
                    }
                )
                
                if response.status_code != 200:
                    logger.error(f"‚ùå n8n workflow error: {response.status_code}")
                    logger.error(f"   Response: {response.text[:500]}")
                    raise Exception(f"n8n workflow failed: {response.status_code}")
                
                result = response.json()
                logger.info(f"‚úÖ n8n –¥–∞–π–¥–∂–µ—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                logger.info(f"   –¢–µ–º: {len(result.get('topics', []))}")
                logger.info(f"   –°–ø–∏–∫–µ—Ä–æ–≤: {len(result.get('speakers_summary', {}))}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç–æ–¥–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                result["generation_method"] = "n8n"
                
                return result
                
        except httpx.TimeoutException:
            logger.error(f"‚è∞ Timeout –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞ ({self.digest_timeout}s)")
            raise Exception(f"n8n workflow timeout after {self.digest_timeout}s")
        
        except httpx.ConnectError as e:
            logger.error(f"üîå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ n8n: {e}")
            logger.error(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ n8n –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É {self.n8n_digest_webhook}")
            raise Exception(f"n8n unavailable: {str(e)}")
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ n8n –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            raise
    
    def _extract_topics_from_langchain_result(self, digest_obj) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ LangChain –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        try:
            # digest_obj —Ç–µ–ø–µ—Ä—å —è–≤–ª—è–µ—Ç—Å—è Pydantic –æ–±—ä–µ–∫—Ç–æ–º SupervisorOutput
            # –¢–µ–º—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ sections.topics –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
            topics_text = digest_obj.sections.topics
            logger.debug(f"üìù –¢–µ–º—ã –∏–∑ LangChain: {topics_text[:200]}...")
            
            # –ü–∞—Ä—Å–∏–º —Ç–µ–º—ã –∏–∑ HTML/—Ç–µ–∫—Å—Ç–∞
            import re
            
            # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏
            clean_text = re.sub(r'<[^>]+>', '', topics_text)
            
            # –ò—â–µ–º —Ç–µ–º—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
            topics = []
            
            # –§–æ—Ä–º–∞—Ç 1: "‚Ä¢ –ù–∞–∑–≤–∞–Ω–∏–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)"
            pattern1 = r'‚Ä¢\s*([^(]+)\s*\([^)]*\)'
            matches1 = re.findall(pattern1, clean_text)
            if matches1:
                topics.extend([topic.strip() for topic in matches1])
            
            # –§–æ—Ä–º–∞—Ç 2: "1. –ù–∞–∑–≤–∞–Ω–∏–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)"
            pattern2 = r'\d+\.\s*([^(]+)\s*\([^)]*\)'
            matches2 = re.findall(pattern2, clean_text)
            if matches2:
                topics.extend([topic.strip() for topic in matches2])
            
            # –§–æ—Ä–º–∞—Ç 3: –ü—Ä–æ—Å—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏—è –±–µ–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
            if not topics:
                # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º –∏ –±–µ—Ä–µ–º –Ω–µ–ø—É—Å—Ç—ã–µ
                lines = [line.strip() for line in clean_text.split('\n') if line.strip()]
                topics = [line for line in lines if not line.startswith(('üéØ', 'üìä', 'üë•', 'üìù'))]
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            unique_topics = []
            seen = set()
            for topic in topics:
                if topic and topic not in seen:
                    unique_topics.append(topic)
                    seen.add(topic)
            
            logger.debug(f"üìù –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç–µ–º: {len(unique_topics)}")
            return unique_topics
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–º: {e}")
            return []
    
    def _extract_speakers_from_langchain_result(self, digest_obj) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ LangChain –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        try:
            # digest_obj —Ç–µ–ø–µ—Ä—å —è–≤–ª—è–µ—Ç—Å—è Pydantic –æ–±—ä–µ–∫—Ç–æ–º SupervisorOutput
            # –°–ø–∏–∫–µ—Ä—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ sections.participants –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
            participants_text = digest_obj.sections.participants
            logger.debug(f"üë• –£—á–∞—Å—Ç–Ω–∏–∫–∏ –∏–∑ LangChain: {participants_text[:200]}...")
            
            # –ü–∞—Ä—Å–∏–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
            import re
            
            speakers_summary = {}
            
            # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏
            clean_text = re.sub(r'<[^>]+>', '', participants_text)
            
            # –§–æ—Ä–º–∞—Ç 1: "@username (—Ä–æ–ª—å, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π)"
            pattern1 = r'@(\w+)\s*\(([^,]+)'
            matches1 = re.findall(pattern1, clean_text)
            for username, role in matches1:
                speakers_summary[username] = role.strip()
            
            # –§–æ—Ä–º–∞—Ç 2: "@username - —Ä–æ–ª—å (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π)"
            pattern2 = r'@(\w+)\s*-\s*([^(]+)'
            matches2 = re.findall(pattern2, clean_text)
            for username, role in matches2:
                speakers_summary[username] = role.strip()
            
            # –§–æ—Ä–º–∞—Ç 3: "‚Ä¢ @username (—Ä–æ–ª—å)"
            pattern3 = r'‚Ä¢\s*@(\w+)\s*\(([^)]+)\)'
            matches3 = re.findall(pattern3, clean_text)
            for username, role in matches3:
                speakers_summary[username] = role.strip()
            
            logger.debug(f"üë• –ò–∑–≤–ª–µ—á–µ–Ω–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {len(speakers_summary)}")
            return speakers_summary
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤: {e}")
            return {}
    
    async def analyze_mention(
        self,
        mentioned_user: str,
        context_messages: List[Message]
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ n8n workflow
        
        Args:
            mentioned_user: Username —É–ø–æ–º—è–Ω—É—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context_messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–¥–æ/–ø–æ—Å–ª–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è)
            
        Returns:
            {
                "context_summary": str,
                "mention_reason": str,
                "urgency": str,
                "key_points": List[str]
            }
        """
        try:
            logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è @{mentioned_user}")
            logger.info(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: {len(context_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è n8n
            formatted_context = []
            for msg in context_messages:
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ username
                username = "Unknown"
                if hasattr(msg, 'sender') and msg.sender:
                    if hasattr(msg.sender, 'username') and msg.sender.username:
                        username = msg.sender.username
                    elif hasattr(msg.sender, 'first_name') and msg.sender.first_name:
                        username = msg.sender.first_name
                
                formatted_context.append({
                    "username": username,
                    "text": msg.text or "",
                    "timestamp": msg.date.isoformat() if hasattr(msg.date, 'isoformat') else str(msg.date)
                })
            
            # –í—ã–∑—ã–≤–∞–µ–º n8n workflow
            async with httpx.AsyncClient(timeout=self.mention_timeout) as client:
                logger.info(f"üì° –í—ã–∑–æ–≤ n8n workflow: {self.n8n_mention_webhook}")
                
                response = await client.post(
                    self.n8n_mention_webhook,
                    json={
                        "mention_context": formatted_context,
                        "mentioned_user": mentioned_user
                    }
                )
                
                if response.status_code != 200:
                    logger.error(f"‚ùå n8n workflow error: {response.status_code}")
                    logger.error(f"   Response: {response.text[:500]}")
                    raise Exception(f"n8n workflow failed: {response.status_code}")
                
                result = response.json()
                logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω")
                logger.info(f"   –°—Ä–æ—á–Ω–æ—Å—Ç—å: {result.get('urgency', 'unknown')}")
                logger.info(f"   –ü—Ä–∏—á–∏–Ω–∞: {result.get('mention_reason', 'unknown')[:50]}...")
                
                return result
                
        except httpx.TimeoutException:
            logger.error(f"‚è∞ Timeout –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è ({self.mention_timeout}s)")
            raise Exception(f"n8n workflow timeout after {self.mention_timeout}s")
        
        except httpx.ConnectError as e:
            logger.error(f"üîå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ n8n: {e}")
            raise Exception(f"n8n unavailable: {str(e)}")
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è: {e}")
            raise
    
    def format_digest_for_telegram(self, digest: Dict[str, Any], group_title: str) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∞–π–¥–∂–µ—Å—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram
        
        V2: –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è V2 pipeline, digest —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≥–æ—Ç–æ–≤—ã–π HTML –≤ –ø–æ–ª–µ digest_html
        V1: –î–µ–ª–µ–≥–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ telegram_formatter
        
        Args:
            digest: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç generate_digest()
            group_title: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ HTML
        """
        # V2 Pipeline: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–π HTML –æ—Ç Supervisor Synthesizer
        if self.use_v2_pipeline and 'digest_html' in digest:
            logger.info("üìÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–π HTML digest –∏–∑ V2 pipeline")
            return digest['digest_html']
        
        # V1 Pipeline: —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ telegram_formatter
        logger.info("üìÑ –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º digest —á–µ—Ä–µ–∑ telegram_formatter (V1)")
        return telegram_formatter.format_digest_for_telegram(digest, group_title)
    
    def format_mention_for_telegram(
        self, 
        analysis: Dict[str, Any], 
        group_title: str,
        message_link: str = None
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ Telegram
        
        –î–µ–ª–µ–≥–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ telegram_formatter –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π
        –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ Markdown ‚Üí MarkdownV2 —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
        
        Args:
            analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç analyze_mention()
            group_title: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            message_link: –°—Å—ã–ª–∫–∞ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ MarkdownV2
        """
        # –ú–∞–ø–ø–∏–Ω–≥ urgency –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å telegram_formatter
        urgency_mapping = {
            "low": "normal",
            "medium": "important",
            "high": "urgent"
        }
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é analysis —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º urgency
        normalized_analysis = analysis.copy()
        old_urgency = normalized_analysis.get('urgency', 'medium')
        normalized_analysis['urgency'] = urgency_mapping.get(old_urgency, 'important')
        
        # –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if 'context_summary' in normalized_analysis:
            normalized_analysis['context'] = normalized_analysis.pop('context_summary')
        
        return telegram_formatter.format_mention_for_telegram(
            normalized_analysis,
            group_title,
            message_link
        )


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
group_digest_generator = GroupDigestGenerator()

