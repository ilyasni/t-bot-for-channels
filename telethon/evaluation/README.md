# Telegram Bot Evaluation System

–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ Telegram –±–æ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAGAS metrics –∏ golden dataset.

## üéØ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

- **Golden Dataset Management** - CRUD –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö Q&A
- **RAGAS Integration** - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏ custom Telegram-specific –º–µ—Ç—Ä–∏–∫–∏
- **Batch Evaluation** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –±–æ–ª—å—à–∏—Ö datasets
- **Langfuse Integration** - traces, scores, A/B testing
- **Prometheus Metrics** - –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
- **Admin Commands** - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Telegram –±–æ—Ç–∞
- **CLI Tools** - –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É
- **Grafana Dashboard** - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫

### üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

**–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ RAGAS –º–µ—Ç—Ä–∏–∫–∏:**
- `AnswerCorrectness` - semantic similarity + factual overlap
- `FactualCorrectness` - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
- `Faithfulness` - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ retrieved context
- `ContextRelevance` - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤

**Telegram-specific custom –º–µ—Ç—Ä–∏–∫–∏:**
- `ChannelContextAwareness` - –ø–æ–Ω–∏–º–∞–µ—Ç –ª–∏ –±–æ—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –∫–∞–Ω–∞–ª–∞
- `GroupSynthesisQuality` - –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ–∑–∞ group discussions
- `MultiSourceCoherence` - —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤
- `ToneAppropriateness` - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–æ–Ω–∞ –∞—É–¥–∏—Ç–æ—Ä–∏–∏

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
telethon/evaluation/
‚îú‚îÄ‚îÄ __init__.py                    # Module exports
‚îú‚îÄ‚îÄ schemas.py                     # Pydantic models
‚îú‚îÄ‚îÄ metrics.py                     # Prometheus metrics
‚îú‚îÄ‚îÄ golden_dataset_manager.py      # CRUD –¥–ª—è golden data
‚îú‚îÄ‚îÄ bot_evaluator.py              # RAGAS integration
‚îú‚îÄ‚îÄ evaluation_runner.py          # Batch evaluation
‚îú‚îÄ‚îÄ langfuse_integration.py       # Langfuse Datasets API
‚îî‚îÄ‚îÄ cli.py                        # Command line tools
```

## üìÅ Golden Datasets

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ dataset

```json
{
  "version": "2.0.0",
  "dataset_name": "automotive_tech_channels_v1",
  "items": [
    {
      "dataset_name": "automotive_tech_channels_v1",
      "item_id": "auto_001",
      "category": "automotive_tech",
      "input": {
        "user_id": 123456,
        "channels": ["@drifting_channel", "@automotive_tech"],
        "context_type": "multi_channel"
      },
      "query": "–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª –¥–ª—è –¥—Ä–∏—Ñ—Ç–∞?",
      "telegram_context": {...},
      "expected_output": "–î–ª—è –¥—Ä–∏—Ñ—Ç–∞ –Ω—É–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å...",
      "retrieved_contexts": [...],
      "metadata": {...},
      "difficulty": "advanced",
      "tone": "technical",
      "requires_multi_source": true
    }
  ]
}
```

### –î–æ—Å—Ç—É–ø–Ω—ã–µ datasets

- **`automotive_tech_channels_v1`** - automotive –∏ tech –∫–∞–Ω–∞–ª—ã (5 items)
- **`team_discussions_groups_v1`** - group discussions (3 items)

## üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### 1. Telegram Admin Commands

```bash
# –ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ datasets
/evaluate_datasets

# –ó–∞–ø—É—Å—Ç–∏—Ç—å evaluation
/evaluate automotive_tech_channels_v1 eval_v1

# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª—å—é
/evaluate automotive_tech_channels_v1 eval_gpt4o openrouter gpt-4o

# –°—Ç–∞—Ç—É—Å evaluation runs
/evaluate_status

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ run
/evaluate_results eval_v1
```

### 2. CLI Tools

```bash
# –°–æ–∑–¥–∞—Ç—å dataset –∏–∑ JSON —Ñ–∞–π–ª–∞
python -m evaluation.cli create-dataset \
    --name "my_dataset" \
    --file "data/golden_qa.json" \
    --sync-langfuse

# –ó–∞–ø—É—Å—Ç–∏—Ç—å evaluation
python -m evaluation.cli run-evaluation \
    --dataset "automotive_tech_channels_v1" \
    --run-name "eval_gpt4o" \
    --model-provider "openrouter" \
    --model-name "gpt-4o" \
    --workers 4

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
python -m evaluation.cli export-results \
    --run-name "eval_gpt4o" \
    --output "results.json"

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ dataset
python -m evaluation.cli dataset-stats \
    --dataset "automotive_tech_channels_v1"
```

### 3. API Endpoints

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å batch evaluation
curl -X POST http://localhost:8020/evaluation/batch \
    -H "Content-Type: application/json" \
    -d '{
        "dataset_name": "automotive_tech_channels_v1",
        "run_name": "eval_api_test",
        "model_provider": "openrouter",
        "model_name": "gpt-4o-mini"
    }'

# –°—Ç–∞—Ç—É—Å evaluation run
curl http://localhost:8020/evaluation/status/eval_api_test

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation run
curl http://localhost:8020/evaluation/results/eval_api_test
```

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### Prometheus Metrics

- `bot_evaluation_answer_correctness` - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ correctness scores
- `bot_evaluation_faithfulness` - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ faithfulness scores
- `bot_evaluation_channel_context_awareness` - channel awareness scores
- `bot_evaluation_group_synthesis_quality` - group synthesis scores
- `bot_evaluation_runs_total` - —Å—á–µ—Ç—á–∏–∫ evaluation runs
- `bot_evaluation_duration_seconds` - –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å evaluation runs
- `bot_evaluation_runs_active` - –∞–∫—Ç–∏–≤–Ω—ã–µ evaluation runs
- `bot_evaluation_items_processed_total` - –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ items

### Grafana Dashboard

Dashboard –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: `http://grafana:3000/d/bot-evaluation`

**–ü–∞–Ω–µ–ª–∏:**
- Answer Correctness Rate over time
- Faithfulness Rate over time
- Average scores by dataset (gauges)
- Evaluation runs total
- Duration percentiles
- Items processed

### Langfuse Integration

- **Datasets** - golden Q&A —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
- **Traces** - –¥–µ—Ç–∞–ª—å–Ω—ã–µ traces –¥–ª—è –∫–∞–∂–¥–æ–≥–æ evaluation
- **Scores** - –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ –∫–∞–∂–¥–æ–º—É item
- **Runs** - A/B testing —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

URL: `https://langfuse.produman.studio/datasets/{dataset_name}/runs/{run_name}`

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### Environment Variables

```bash
# Evaluation metrics
EVALUATION_METRICS_ENABLED=true

# Langfuse integration
LANGFUSE_URL=http://localhost:3000

# Evaluation settings
EVALUATION_ITEM_TIMEOUT=300
EVALUATION_PARALLEL_WORKERS=4
EVALUATION_DEFAULT_MODEL_PROVIDER=openrouter
EVALUATION_DEFAULT_MODEL_NAME=gpt-4o-mini
```

### Database Schema

```sql
-- Golden dataset items
CREATE TABLE evaluation_golden_dataset (
    id SERIAL PRIMARY KEY,
    dataset_name VARCHAR(255) NOT NULL,
    item_id VARCHAR(255) UNIQUE NOT NULL,
    category VARCHAR(100) NOT NULL,
    input_data JSONB NOT NULL,
    query TEXT NOT NULL,
    telegram_context JSONB NOT NULL,
    expected_output TEXT NOT NULL,
    retrieved_contexts JSONB,
    metadata JSONB DEFAULT '{}',
    difficulty VARCHAR(20),
    tone VARCHAR(20),
    requires_multi_source BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Evaluation runs
CREATE TABLE evaluation_runs (
    id SERIAL PRIMARY KEY,
    run_name VARCHAR(255) UNIQUE NOT NULL,
    dataset_name VARCHAR(255) NOT NULL,
    model_provider VARCHAR(50) NOT NULL,
    model_name VARCHAR(100) NOT NULL,
    status VARCHAR(20) DEFAULT 'pending',
    progress FLOAT DEFAULT 0.0,
    total_items INTEGER DEFAULT 0,
    processed_items INTEGER DEFAULT 0,
    avg_score FLOAT,
    scores JSONB,
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ
);

-- Evaluation results
CREATE TABLE evaluation_results (
    id SERIAL PRIMARY KEY,
    run_id INTEGER REFERENCES evaluation_runs(id),
    item_id VARCHAR(255) NOT NULL,
    query TEXT NOT NULL,
    expected_output TEXT NOT NULL,
    actual_output TEXT NOT NULL,
    answer_correctness FLOAT,
    faithfulness FLOAT,
    context_relevance FLOAT,
    channel_context_awareness FLOAT,
    group_synthesis_quality FLOAT,
    multi_source_coherence FLOAT,
    tone_appropriateness FLOAT,
    overall_score FLOAT,
    evaluated_at TIMESTAMPTZ DEFAULT NOW()
);
```

## üîß –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫

1. –°–æ–∑–¥–∞—Ç—å custom AspectCritic –≤ `bot_evaluator.py`:

```python
def _create_custom_metric(self) -> AspectCritic:
    return AspectCritic(
        name="custom_metric_name",
        definition="""
        –û–ø–∏—Å–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏...
        –í–µ—Ä–Ω–∏—Ç–µ 1.0 –µ—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ, 0.0 –∏–Ω–∞—á–µ.
        """,
        llm=self.ragas_llm
    )
```

2. –î–æ–±–∞–≤–∏—Ç—å –≤ —Å–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –≤ `_setup_ragas_metrics()`

3. –î–æ–±–∞–≤–∏—Ç—å Prometheus metric –≤ `metrics.py`

4. –û–±–Ω–æ–≤–∏—Ç—å —Å—Ö–µ–º—ã –≤ `schemas.py`

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö datasets

1. –°–æ–∑–¥–∞—Ç—å JSON —Ñ–∞–π–ª –≤ `data/` –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CLI –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞:

```bash
python -m evaluation.cli create-dataset \
    --name "new_dataset" \
    --file "data/new_dataset.json"
```

### –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ API

–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ endpoints –≤ `rag_service/main.py`:

```python
@app.get("/evaluation/custom-endpoint")
async def custom_endpoint():
    # Implementation
    pass
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ó–∞–ø—É—Å–∫ evaluation

```bash
# –¢–µ—Å—Ç –Ω–∞ sample dataset
python -m evaluation.cli run-evaluation \
    --dataset "automotive_tech_channels_v1" \
    --run-name "test_run" \
    --workers 2 \
    --timeout 60
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç—Ä–∏–∫

```bash
# Prometheus metrics
curl http://localhost:9090/metrics | grep bot_evaluation

# Grafana dashboard
open http://grafana:3000/d/bot-evaluation
```

### Langfuse traces

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å traces –≤ Langfuse UI
open https://langfuse.produman.studio/datasets
```

## üìà Best Practices

### Golden Dataset

- **–ö–∞—á–µ—Å—Ç–≤–æ over –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ** - –ª—É—á—à–µ 10 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö items —á–µ–º 100 –ø–ª–æ—Ö–∏—Ö
- **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ** - –≤–∫–ª—é—á–∞–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, —Ç–∏–ø—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- **–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å** - —Ä–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ dataset
- **–í–∞–ª–∏–¥–∞—Ü–∏—è** - –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ expected_output –≤—Ä—É—á–Ω—É—é

### Evaluation

- **A/B testing** - —Å—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–º dataset
- **–†–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å** - –∑–∞–ø—É—Å–∫–∞–π—Ç–µ evaluation –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –±–æ—Ç–µ
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ —Ç—Ä–µ–Ω–¥—ã –∫–∞—á–µ—Å—Ç–≤–∞
- **–ê–Ω–∞–ª–∏–∑** - –∏–∑—É—á–∞–π—Ç–µ –æ—à–∏–±–∫–∏ –∏ —É–ª—É—á—à–∞–π—Ç–µ —Å–∏—Å—Ç–µ–º—É

### Production

- **Graceful degradation** - —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–∞–∂–µ –µ—Å–ª–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
- **Resource limits** - –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–π—Ç–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å –∏ timeout
- **Error handling** - –ª–æ–≥–∏—Ä—É–π—Ç–µ –æ—à–∏–±–∫–∏ –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Ä–∞–±–æ—Ç—É
- **Security** - –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ admin –∫–æ–º–∞–Ω–¥–∞–º

## üêõ Troubleshooting

### RAGAS –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫—É
pip list | grep ragas

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–º–ø–æ—Ä—Ç—ã
python -c "from ragas import evaluate; print('RAGAS OK')"
```

### Langfuse –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å
docker ps | grep langfuse

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker logs langfuse-web
```

### Evaluation fails

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker logs telethon | grep evaluation

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å database
docker exec -it postgres psql -U postgres -d postgres -c "SELECT * FROM evaluation_runs;"
```

### Prometheus metrics –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
echo $EVALUATION_METRICS_ENABLED

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å endpoint
curl http://localhost:8020/metrics | grep bot_evaluation
```

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [RAGAS Documentation](https://docs.ragas.io/)
- [Langfuse Documentation](https://langfuse.com/docs)
- [Prometheus Client Python](https://github.com/prometheus/client_python)
- [Telegram Bot API](https://core.telegram.org/bots/api)

## ü§ù Contributing

1. –°–æ–∑–¥–∞–π—Ç–µ feature branch
2. –î–æ–±–∞–≤—å—Ç–µ —Ç–µ—Å—Ç—ã –¥–ª—è –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
3. –û–±–Ω–æ–≤–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
4. –°–æ–∑–¥–∞–π—Ç–µ pull request

## üìÑ License

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—É –∂–µ –ª–∏—Ü–µ–Ω–∑–∏—é —á—Ç–æ –∏ –æ—Å–Ω–æ–≤–Ω–æ–π Telegram Bot –ø—Ä–æ–µ–∫—Ç.
