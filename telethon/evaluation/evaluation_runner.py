"""
Evaluation Runner –¥–ª—è batch evaluation

–ó–∞–ø—É—Å–∫–∞–µ—Ç batch evaluation –Ω–∞ golden dataset —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- Parallel processing –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
- Progress tracking
- Error handling –∏ retry logic
- Integration —Å RAG service
- Prometheus metrics
- Database persistence

Best practices:
- Async/await –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- Graceful degradation –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
- Progress tracking –¥–ª—è –±–æ–ª—å—à–∏—Ö datasets
- Retry logic –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–±–æ–µ–≤
- Resource management (connection pooling)
"""

import asyncio
import logging
import time
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, AsyncGenerator
import json

import httpx
from .golden_dataset_manager import get_golden_dataset_manager
from .bot_evaluator import get_bot_evaluator
from .schemas import (
    EvaluationRun,
    EvaluationResult,
    GoldenDatasetItem
)
from .metrics import (
    increment_counter,
    increment_gauge,
    decrement_gauge,
    set_gauge,
    observe_score,
    evaluation_duration_seconds,
    evaluation_items_processed,
    evaluation_runs_active,
    evaluation_runs_total,
    log_evaluation_metric_error
)

logger = logging.getLogger(__name__)


class EvaluationRunner:
    """Runner –¥–ª—è batch evaluation"""
    
    def __init__(self, rag_service_url: str = "http://localhost:8020"):
        """
        Initialize Evaluation Runner
        
        Args:
            rag_service_url: URL RAG service –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –±–æ—Ç–∞
        """
        self.rag_service_url = rag_service_url
        self.http_client: Optional[httpx.AsyncClient] = None
        
    async def __aenter__(self):
        """Async context manager entry"""
        self.http_client = httpx.AsyncClient(
            timeout=httpx.Timeout(300.0),  # 5 minutes timeout
            limits=httpx.Limits(max_keepalive_connections=10, max_connections=20)
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.http_client:
            await self.http_client.aclose()
    
    async def run_evaluation(
        self,
        dataset_name: str,
        run_name: str,
        model_provider: str = "openrouter",
        model_name: str = "gpt-4o-mini",
        parallel_workers: int = 4,
        timeout_seconds: int = 300
    ) -> EvaluationRun:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å batch evaluation
        
        Args:
            dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ golden dataset
            run_name: –ù–∞–∑–≤–∞–Ω–∏–µ run
            model_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä –º–æ–¥–µ–ª–∏
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            parallel_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤
            timeout_seconds: Timeout –¥–ª—è –æ–¥–Ω–æ–≥–æ evaluation
            
        Returns:
            EvaluationRun —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        start_time = time.time()
        
        # –°–æ–∑–¥–∞—Ç—å EvaluationRun record
        evaluation_run = EvaluationRun(
            run_name=run_name,
            dataset_name=dataset_name,
            model_provider=model_provider,
            model_name=model_name,
            parallel_workers=parallel_workers,
            timeout_seconds=timeout_seconds,
            status="running",
            started_at=datetime.now(timezone.utc),
            progress=0.0
        )
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î
        run_id = await self._save_evaluation_run(evaluation_run)
        
        # Update Prometheus metrics
        increment_gauge(evaluation_runs_active)
        
        try:
            # –ü–æ–ª—É—á–∏—Ç—å items –∏–∑ dataset
            dataset_manager = await get_golden_dataset_manager()
            items = await dataset_manager.get_dataset_items(dataset_name)
            
            if not items:
                raise ValueError(f"No items found in dataset '{dataset_name}'")
            
            evaluation_run.total_items = len(items)
            await self._update_evaluation_run(run_id, evaluation_run)
            
            logger.info(f"üöÄ Starting evaluation run '{run_name}': {len(items)} items, {parallel_workers} workers")
            
            # –ó–∞–ø—É—Å—Ç–∏—Ç—å parallel evaluation
            results = []
            successful_items = 0
            failed_items = 0
            
            # –°–æ–∑–¥–∞—Ç—å semaphore –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏
            semaphore = asyncio.Semaphore(parallel_workers)
            
            # –°–æ–∑–¥–∞—Ç—å tasks –¥–ª—è –≤—Å–µ—Ö items
            tasks = []
            for i, item in enumerate(items):
                task = asyncio.create_task(
                    self._evaluate_single_item_with_semaphore(
                        semaphore, item, model_provider, model_name, timeout_seconds
                    )
                )
                tasks.append((i, task))
            
            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            for i, task in tasks:
                try:
                    result = await task
                    results.append(result)
                    
                    if result.error_message:
                        failed_items += 1
                        increment_counter(
                            evaluation_items_processed,
                            {"dataset": dataset_name, "model_provider": model_provider, "status": "error"}
                        )
                    else:
                        successful_items += 1
                        increment_counter(
                            evaluation_items_processed,
                            {"dataset": dataset_name, "model_provider": model_provider, "status": "success"}
                        )
                    
                    # Update progress
                    processed_items = successful_items + failed_items
                    progress = processed_items / len(items)
                    evaluation_run.processed_items = processed_items
                    evaluation_run.progress = progress
                    
                    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ë–î
                    await self._save_evaluation_result(run_id, result)
                    
                    # Update run progress
                    if processed_items % 5 == 0 or processed_items == len(items):
                        await self._update_evaluation_run(run_id, evaluation_run)
                        logger.info(f"üìä Progress: {processed_items}/{len(items)} ({progress:.1%})")
                
                except Exception as e:
                    logger.error(f"‚ùå Task failed for item {i}: {e}")
                    failed_items += 1
                    increment_counter(
                        evaluation_items_processed,
                        {"dataset": dataset_name, "model_provider": model_provider, "status": "error"}
                    )
            
            # –í—ã—á–∏—Å–ª–∏—Ç—å –∏—Ç–æ–≥–æ–≤—ã–µ scores
            avg_scores = self._calculate_average_scores(results)
            overall_score = avg_scores.get('overall_score', 0.0)
            
            # –ó–∞–≤–µ—Ä—à–∏—Ç—å evaluation run
            evaluation_run.status = "completed"
            evaluation_run.completed_at = datetime.now(timezone.utc)
            evaluation_run.successful_items = successful_items
            evaluation_run.failed_items = failed_items
            evaluation_run.avg_score = overall_score
            evaluation_run.scores = avg_scores
            evaluation_run.progress = 1.0
            
            await self._update_evaluation_run(run_id, evaluation_run)
            
            # Update Prometheus metrics
            duration = time.time() - start_time
            observe_score(
                evaluation_duration_seconds,
                duration,
                {"dataset": dataset_name, "model_provider": model_provider}
            )
            
            increment_counter(
                evaluation_runs_total,
                {"dataset": dataset_name, "model_provider": model_provider, "status": "completed"}
            )
            
            logger.info(f"‚úÖ Evaluation run '{run_name}' completed:")
            logger.info(f"   Items: {successful_items} successful, {failed_items} failed")
            logger.info(f"   Overall score: {overall_score:.3f}")
            logger.info(f"   Duration: {duration:.1f}s")
            
            return evaluation_run
            
        except Exception as e:
            logger.error(f"‚ùå Evaluation run '{run_name}' failed: {e}")
            
            # Update status to failed
            evaluation_run.status = "failed"
            evaluation_run.completed_at = datetime.now(timezone.utc)
            await self._update_evaluation_run(run_id, evaluation_run)
            
            # Update Prometheus metrics
            increment_counter(
                evaluation_runs_total,
                {"dataset": dataset_name, "model_provider": model_provider, "status": "failed"}
            )
            
            raise
            
        finally:
            # Update Prometheus metrics
            decrement_gauge(evaluation_runs_active)
    
    async def _evaluate_single_item_with_semaphore(
        self,
        semaphore: asyncio.Semaphore,
        item: GoldenDatasetItem,
        model_provider: str,
        model_name: str,
        timeout_seconds: int
    ) -> EvaluationResult:
        """Evaluate single item —Å semaphore –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏"""
        async with semaphore:
            return await self._evaluate_single_item(item, model_provider, model_name, timeout_seconds)
    
    async def _evaluate_single_item(
        self,
        item: GoldenDatasetItem,
        model_provider: str,
        model_name: str,
        timeout_seconds: int
    ) -> EvaluationResult:
        """Evaluate single item"""
        try:
            # –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç RAG service
            actual_output = await self._get_bot_response(item)
            
            # –ü–æ–ª—É—á–∏—Ç—å retrieved contexts (–µ—Å–ª–∏ –µ—Å—Ç—å)
            retrieved_contexts = await self._get_retrieved_contexts(item)
            
            # –ó–∞–ø—É—Å—Ç–∏—Ç—å evaluation
            evaluator = get_bot_evaluator(model_provider, model_name)
            result = await evaluator.evaluate_single_item(
                item=item,
                actual_output=actual_output,
                retrieved_contexts=retrieved_contexts
            )
            
            return result
            
        except asyncio.TimeoutError:
            logger.error(f"‚è∞ Timeout for item {item.item_id}")
            return self._create_error_result(item, f"Timeout after {timeout_seconds}s")
            
        except Exception as e:
            logger.error(f"‚ùå Error evaluating item {item.item_id}: {e}")
            return self._create_error_result(item, str(e))
    
    async def _get_bot_response(self, item: GoldenDatasetItem) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç RAG service"""
        if not self.http_client:
            raise RuntimeError("HTTP client not initialized")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è RAG service
        request_data = {
            "query": item.query,
            "user_id": item.telegram_context.user_id,
            "channels": item.telegram_context.channels,
            "context_type": item.telegram_context.context_type.value
        }
        
        if item.telegram_context.group_id:
            request_data["group_id"] = item.telegram_context.group_id
        
        # –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å
        response = await self.http_client.post(
            f"{self.rag_service_url}/rag/ask",
            json=request_data,
            timeout=60.0
        )
        response.raise_for_status()
        
        result = response.json()
        return result.get("answer", "No answer received")
    
    async def _get_retrieved_contexts(self, item: GoldenDatasetItem) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å retrieved contexts –¥–ª—è RAG evaluation"""
        if not self.http_client:
            return []
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è search
            request_data = {
                "query": item.query,
                "user_id": item.telegram_context.user_id,
                "channels": item.telegram_context.channels,
                "limit": 5
            }
            
            # –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å
            response = await self.http_client.post(
                f"{self.rag_service_url}/rag/search",
                json=request_data,
                timeout=30.0
            )
            response.raise_for_status()
            
            result = response.json()
            contexts = []
            
            for doc in result.get("documents", []):
                if "content" in doc:
                    contexts.append(doc["content"])
                elif "text" in doc:
                    contexts.append(doc["text"])
            
            return contexts
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get retrieved contexts for {item.item_id}: {e}")
            return []
    
    def _calculate_average_scores(self, results: List[EvaluationResult]) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ä–µ–¥–Ω–∏–µ scores –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not results:
            return {}
        
        successful_results = [r for r in results if not r.error_message]
        if not successful_results:
            return {}
        
        scores = {}
        metrics = [
            'answer_correctness', 'faithfulness', 'context_relevance',
            'factual_correctness', 'channel_context_awareness',
            'group_synthesis_quality', 'multi_source_coherence',
            'tone_appropriateness', 'overall_score'
        ]
        
        for metric in metrics:
            values = []
            for result in successful_results:
                if hasattr(result.metrics, metric):
                    value = getattr(result.metrics, metric)
                    if value is not None:
                        values.append(value)
            
            if values:
                scores[metric] = sum(values) / len(values)
        
        return scores
    
    def _create_error_result(self, item: GoldenDatasetItem, error_message: str) -> EvaluationResult:
        """–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π"""
        from .schemas import EvaluationMetrics
        
        fallback_metrics = EvaluationMetrics(
            answer_correctness=0.0,
            faithfulness=0.0,
            context_relevance=0.0,
            factual_correctness=0.0,
            channel_context_awareness=0.0,
            group_synthesis_quality=0.0,
            multi_source_coherence=0.0,
            tone_appropriateness=0.0,
            overall_score=0.0,
            evaluation_time=0.0,
            model_used="error"
        )
        
        return EvaluationResult(
            item_id=item.item_id,
            query=item.query,
            expected_output=item.expected_output,
            actual_output="",
            metrics=fallback_metrics,
            telegram_context=item.telegram_context,
            error_message=error_message
        )
    
    async def _save_evaluation_run(self, evaluation_run: EvaluationRun) -> int:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å evaluation run –≤ –ë–î"""
        # TODO: Implement database save
        # For now, return a mock ID
        return 1
    
    async def _update_evaluation_run(self, run_id: int, evaluation_run: EvaluationRun):
        """–û–±–Ω–æ–≤–∏—Ç—å evaluation run –≤ –ë–î"""
        # TODO: Implement database update
        pass
    
    async def _save_evaluation_result(self, run_id: int, result: EvaluationResult):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å evaluation result –≤ –ë–î"""
        # TODO: Implement database save
        pass


# ============================================================================
# Utility Functions
# ============================================================================

async def run_evaluation_batch(
    dataset_name: str,
    run_name: str,
    model_provider: str = "openrouter",
    model_name: str = "gpt-4o-mini",
    parallel_workers: int = 4,
    timeout_seconds: int = 300
) -> EvaluationRun:
    """
    Utility function –¥–ª—è –∑–∞–ø—É—Å–∫–∞ batch evaluation
    
    Args:
        dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ dataset
        run_name: –ù–∞–∑–≤–∞–Ω–∏–µ run
        model_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä –º–æ–¥–µ–ª–∏
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        parallel_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤
        timeout_seconds: Timeout
        
    Returns:
        EvaluationRun —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    async with EvaluationRunner() as runner:
        return await runner.run_evaluation(
            dataset_name=dataset_name,
            run_name=run_name,
            model_provider=model_provider,
            model_name=model_name,
            parallel_workers=parallel_workers,
            timeout_seconds=timeout_seconds
        )
